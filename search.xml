<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大模型决策</title>
      <link href="/2024/03/28/%E6%9D%8E%E5%AD%A6%E9%BE%99%E8%B0%83%E7%A0%94/"/>
      <url>/2024/03/28/%E6%9D%8E%E5%AD%A6%E9%BE%99%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<h1 id="李学龙调研"><a href="#李学龙调研" class="headerlink" title="李学龙调研"></a>李学龙调研</h1><p>找了他最近发的很多论文，但是找到可能和大模型决策有关的好像还挺少的<br>这个老师好像主要是视觉、特征学习、量子计算、图神经网络还有一些遥感方向的东西,而且有很多无监督学习</p><p>他还有一个<a href="https://www.nwpu.edu.cn/info/1198/73108.htm">无人机的自主“聊天群”</a>提出“群聊式”对话交互方法，将声音、图像和无人机自身状态等多种信息，通过大模型转换为自然语言的对话形式，实现了用户与无人机，以及无人机与无人机之间自主和直观的交互方式。</p><p>可能这个多种智能体协调的跟这个是一个意思？用大模型进行文本交流.<br>有一个文章链接是关于李学龙教授的临地安防的，里面可能有更细致的内容但是看这个要会员。</p><p>有个关于这个新闻的评论<a href="https://zhuanlan.zhihu.com/p/652397860">异构智能体自主协作，大模型扮演了什么角色？</a>这个里面有对李学龙教授的采访，其中的重点我认为依然是大模型本身的问题可能会带到下游任务中，比如一些伦理道德隐私之类的。</p><p>关于近期能找到相关的论文，首先就是这个重磅的CCFA的论文，讲的无人机<a href="https://link.springer.com/article/10.1007/s11432-023-3917-2">Optics-driven drone</a></p><blockquote><p>本文讨论了一种新型的光学驱动无人机（ODD）系统，该系统基于人工智能和激光无线电力传输技术，旨在解决当前无人机由于化学电池能量密度限制而面临的续航能力短，任务覆盖范围和信息感知能力有限等问题。ODD系统可以在飞行中实时报告其位置和任务执行状态给地面站，当需要补充能量时，地面站使用智能视觉跟踪和指向算法来实现稳定的获取、跟踪和指向，然后激活激光向无人机传输能量。无人机上的光电转换模块将接收到的激光能量转换为电能，完成稳定高效的远距离无线能量补给。该系统面临的特定挑战包括目标跟踪、大气湍流缓解和障碍物感知。</p><p>为解决这些问题，研究首先提出了一种智能视觉跟踪和指向方法，通过嵌入一个光照敏感模块和一个空间尺度跟踪模块来提高跟踪稳定性，并使用基于金字塔表示的判别相关滤波器来处理复杂图像序列中的大尺度变化。此外，为了缓解大气湍流效应，研究提出了一种基于光斑反馈强化学习的自适应光束整形方法。最后，为确保激光链接的安全，研究提出了一种遮挡检测和激光功率自主调整方法，通过编码和调制激光束，并使用飞行时间方法来估计激光束的传播距离，从而在遇到障碍物时减少激光功率或关闭激光。</p><p>通过实验验证，研究展示了提出系统的有效性和优越性，为未来无人机系统的发展提供了参考。这些技术的应用预计将使ODD在未来能够实现全天候（昼夜）、远距离（千米级）和自主的无线能量补充，支持无人机在广阔未知环境中进行更复杂的自主智能任务。此外，ODD系统的技术优势还将支持无人机群体通过地对空一对多无线充电支持协同任务，进一步放大其优势和效率</p></blockquote><p>然后是<a href="https://link.springer.com/article/10.1007/s40747-023-01316-9">A communication-based identification of critical drones in malicious drone swarm networks</a></p><blockquote><p>这篇论文提出了一种基于通信的关键无人机识别方法，用于恶意无人机群的网络。在面对日益增加的恶意无人机群攻击时，准确识别关键的恶意无人机对于优化定向能量攻击并最大化其效果至关重要。然而，当前关于关键无人机识别的研究仍处于初级阶段，几乎完全依赖于不考虑无人机群分布特性的传统中心性方法。这导致关键无人机的识别不准确，进而降低了定向能量攻击的效率。为了解决这一问题，本文提出了一种新的基于无人机分布特性、通信强度和通信规模的关键无人机识别方法。该方法首先构建了一个基于3D位置和交互范围的无人机群动态通信预测网络模型，以预测无人机之间的动态通信。然后，本文提出了一种新的基于动态巨大连通分量（GCC）的规模强度中心性（DGSIC）方法，结合了动态通信预测网络的局部、全局和社区结构，以识别具有更强通信能力的关键节点。此外，通过动态策略，在每一步迭代中识别一个关键节点，考虑网络配置的演变，确保识别的节点在当前网络中仍是最关键的。此外，还采用了优先策略，优先识别在GCC中可以显著影响网络连通性和通信的节点。DGSIC优化了定向能量攻击的攻击顺序，有助于快速瓦解恶意无人机群。通过在四个模拟网络和八个真实世界网络上的广泛实验，证明了DGSIC在鲁棒性和级联故障性能方面的优越性。</p></blockquote><p>Pessimistic value iteration for multi-task data sharing in Offline Reinforcement Learning （没找到原文，只看到了介绍）</p><blockquote><p>本文研究了在离线强化学习（Offline Reinforcement Learning）中进行多任务数据共享的方法。尽管离线强化学习已经在从固定数据集中学习特定任务的策略方面显示出良好的效果，但其成功往往严重依赖于给定数据集的覆盖范围和质量。在特定任务数据集有限的情况下，一种自然的方法是利用其他任务的数据集来改进离线强化学习，即进行多任务数据共享（Multi-Task Data Sharing，MTDS）。然而，直接共享其他任务的数据集会加剧离线强化学习中分布偏移的问题。</p><p>为了解决这个问题，本文提出了一种基于不确定性的MTDS方法，该方法不需要数据选择，可以直接共享整个数据集。在基于集合的不确定性量化基础上，他们对共享的离线数据集进行悲观的值迭代，为单任务和多任务的离线强化学习提供了一个统一的框架。此外，还提供了理论分析，证明了方法的最优性差距只与共享数据集的期望数据覆盖范围有关，从而解决了数据共享中的分布偏移问题。</p><p>在实验方面，他们发布了一个MTDS基准，并从三个具有挑战性的领域收集了数据集。实验结果表明，在具有挑战性的MTDS问题中，算法优于之前的最先进方法。</p></blockquote><p>论文确实好少，我就找了其它一些资料</p><p>一个GitHub仓库里面列举了很多大模型和多智能体相关的文章，<a href="https://github.com/AGI-Edgerunners/LLM-Agents-Papers">仓库在这里</a></p><p><a href="https://arxiv.org/pdf/2310.20151.pdf">Multi-Agent Consensus Seeking via Large Language Models</a></p><blockquote><p>这篇论文研究了基于大型语言模型（LLMs）的多智能体系统在协作中达成共识的概念。研究发现LLM驱动的智能体在共识寻求中主要采用平均策略，同时分析了智能体数量、个性和网络拓扑对谈判过程的影响。实验结果显示，增加智能体数量可以减轻系统的幻觉，稳定群体决策。</p></blockquote><p><a href="https://arxiv.org/abs/2402.01680">Large Language Model based Multi-Agents: A Survey of Progress and Challenges</a></p><blockquote><p>一篇综述，如果有必要的会仔细看看扩充一下知识面</p></blockquote><p><a href="https://arxiv.org/abs/2311.13884">Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach</a></p><blockquote><p>这篇论文介绍了一个名为LLaMAC的框架，旨在增强大规模多Agent环境中基于大型语言模型的代理的协调和决策能力。该框架从多Agent强化学习中的actor-critic框架中汲取灵感，通过模块化和高效利用token的解决方案有效地应对了语言模型和多Agent系统中的挑战。</p></blockquote><p><a href="https://arxiv.org/abs/2308.08155">AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a></p><blockquote><p>这是一个能利用大模型构建多智能体的框架,允许开发人员构建通过多个代理进行对话以完成任务的LLM应用。AutoGen代理是可定制的、可对话的，并且可以在使用LLMs、人类输入和工具的各种模式下运行。</p></blockquote><p><a href="https://arxiv.org/abs/2306.03314">MULTI-AGENT COLLABORATION: HARNESSING THE POWER OF INTELLIGENT LLM AGENTS</a></p><blockquote><p>该论文提出了一种通过利用多智能代理系统的力量来增强大型语言模型（LLMs）能力的框架,引入了一个协作环境，多个智能代理组件共同处理复杂任务，提高效率。多智能代理系统中的各个代理相互协作，共同实现共享目标.</p></blockquote><p><a href="https://arxiv.org/abs/2401.01312">LLM HARMONY: MULTI-AGENT COMMUNICATION FOR PROBLEM SOLVING</a></p><blockquote><p>这篇论文介绍了一种新颖的多智能体通信框架，旨在增强大型语言模型（LLMs）在自主问题解决方面的能力。该框架利用多个具有不同角色的LLM代理进行角色扮演通信，为处理各种问题场景提供了一种细致且灵活的方法。</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 大模型论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>智能计算实验二</title>
      <link href="/2024/03/27/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%AE%9E%E9%AA%8C%E4%BA%8C/"/>
      <url>/2024/03/27/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%AE%9E%E9%AA%8C%E4%BA%8C/</url>
      
        <content type="html"><![CDATA[<p>周三上午在写多智能体作业<br>先睡午觉晚上再说，等会上课调研一下那个李学龙的东西</p><p>周四上午在写风格迁移作业，打算周末的时候好好看下代码，最好是能够自己实现一遍<br>今天下午就可以调研一下大模型决策这种东西。</p><h1 id="卷积层的实现"><a href="#卷积层的实现" class="headerlink" title="卷积层的实现"></a>卷积层的实现</h1><p>这是卷积层的前向传播的原始实现，使用了4层循环，时间复杂度相当高</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_raw</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    self.<span class="built_in">input</span> = <span class="built_in">input</span> <span class="comment"># [N, C, H, W]</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;padding img and let the center be img&quot;&quot;&quot;</span></span><br><span class="line">    height = self.<span class="built_in">input</span>.shape[<span class="number">2</span>] + self.padding * <span class="number">2</span></span><br><span class="line">    width = self.<span class="built_in">input</span>.shape[<span class="number">3</span>] + self.padding * <span class="number">2</span></span><br><span class="line">    self.input_pad = np.zeros([self.<span class="built_in">input</span>.shape[<span class="number">0</span>], self.<span class="built_in">input</span>.shape[<span class="number">1</span>], height, width])</span><br><span class="line">    self.input_pad[:, :, self.padding:self.padding+self.<span class="built_in">input</span>.shape[<span class="number">2</span>], self.padding:self.padding+self.<span class="built_in">input</span>.shape[<span class="number">3</span>]] = self.<span class="built_in">input</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;compute the output shape&quot;&quot;&quot;</span></span><br><span class="line">    height_out = (height - self.kernel_size) // self.stride + <span class="number">1</span></span><br><span class="line">    width_out = (width - self.kernel_size) // self.stride + <span class="number">1</span></span><br><span class="line">    self.output = np.zeros([self.<span class="built_in">input</span>.shape[<span class="number">0</span>], self.channel_out, height_out, width_out])</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    卷积计算的核心，输出特征图的形状是（batch, out_channel, height_out, width_out），在这个形状上一个一个遍历</span></span><br><span class="line"><span class="string">    这边注意一下是怎么计算卷积核一个channel产生的特征图的，</span></span><br><span class="line"><span class="string">    首先先选取卷积核该channel上的权重，这个权重的形状应该是（in_channel, kernel_size, kernel_size）</span></span><br><span class="line"><span class="string">    self.weight[:, :, :, idxc]的作用就是选出一个卷积核，因为一个channel_out就是一个卷积核</span></span><br><span class="line"><span class="string">    结果需要计算卷积核与第idxn个输入特征图的对应位置上进行内积并加上偏置</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> idxn <span class="keyword">in</span> <span class="built_in">range</span>(self.<span class="built_in">input</span>.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> idxc <span class="keyword">in</span> <span class="built_in">range</span>(self.channel_out):</span><br><span class="line">            <span class="keyword">for</span> idxh <span class="keyword">in</span> <span class="built_in">range</span>(height_out):</span><br><span class="line">                <span class="keyword">for</span> idxw <span class="keyword">in</span> <span class="built_in">range</span>(width_out):</span><br><span class="line">                    <span class="comment"># <span class="doctag">TODO:</span> 计算卷积层的前向传播，特征图与卷积核的内积再加偏置</span></span><br><span class="line">                    self.output[idxn, idxc, idxh, idxw] = np.<span class="built_in">sum</span>(self.weight[:, :, :, idxc] * self.input_pad[idxn, :, idxh*self.stride:idxh*self.stride+self.kernel_size, idxw*self.stride:idxw*self.stride+self.kernel_size]) + self.bias[idxc]</span><br><span class="line">    self.forward_time = time.time() - start_time</span><br><span class="line">    <span class="keyword">return</span> self.output</span><br></pre></td></tr></table></figure><p>然后就是卷积层的反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_raw</span>(<span class="params">self, top_diff</span>):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="string">&quot;&quot;&quot;make temp of dweight and dbias and bottom_diff&quot;&quot;&quot;</span></span><br><span class="line">    self.d_weight = np.zeros(self.weight.shape)</span><br><span class="line">    self.d_bias = np.zeros(self.bias.shape)</span><br><span class="line">    bottom_diff = np.zeros(self.input_pad.shape)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;compute the gradient of weight and bias&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> idxn <span class="keyword">in</span> <span class="built_in">range</span>(top_diff.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> idxc <span class="keyword">in</span> <span class="built_in">range</span>(top_diff.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> idxh <span class="keyword">in</span> <span class="built_in">range</span>(top_diff.shape[<span class="number">2</span>]):</span><br><span class="line">                <span class="keyword">for</span> idxw <span class="keyword">in</span> <span class="built_in">range</span>(top_diff.shape[<span class="number">3</span>]):</span><br><span class="line">                    <span class="comment"># TODO： 计算卷积层的反向传播， 权重、偏置的梯度和本层损失</span></span><br><span class="line">                    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">                    偏置更新只需要求和对应位置上的梯度即可也就是top_diff</span></span><br><span class="line"><span class="string">                    权重更新也是求和对应位置上梯度与input_pad相乘</span></span><br><span class="line"><span class="string">                    &quot;&quot;&quot;</span></span><br><span class="line">                    self.d_weight[:, :, :, idxc] += top_diff[idxn, idxc, idxh, idxw] * self.input_pad[idxn, :, idxh*self.stride:idxh*self.stride+self.kernel_size, idxw*self.stride:idxw*self.stride+self.kernel_size]</span><br><span class="line">                    self.d_bias[idxc] += top_diff[idxn, idxc, idxh, idxw]</span><br><span class="line">                    bottom_diff[idxn, :, idxh*self.stride:idxh*self.stride+self.kernel_size, idxw*self.stride:idxw*self.stride+self.kernel_size] += top_diff[idxn, idxc, idxh, idxw] * self.weight[:, :, :, idxc]</span><br><span class="line">    bottom_diff = bottom_diff[:, :, self.padding:self.padding+self.<span class="built_in">input</span>.shape[<span class="number">2</span>], self.padding:self.padding+self.<span class="built_in">input</span>.shape[<span class="number">3</span>]]</span><br><span class="line">    self.backward_time = time.time() - start_time</span><br><span class="line">    <span class="keyword">return</span> bottom_diff</span><br></pre></td></tr></table></figure><h1 id="池化层的实现"><a href="#池化层的实现" class="headerlink" title="池化层的实现"></a>池化层的实现</h1><p>这是最大池化层的原始实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_raw</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="string">&quot;&quot;&quot;compute output shape and make max index temp&quot;&quot;&quot;</span></span><br><span class="line">    self.<span class="built_in">input</span> = <span class="built_in">input</span> <span class="comment"># [N, C, H, W]</span></span><br><span class="line">    self.max_index = np.zeros(self.<span class="built_in">input</span>.shape)</span><br><span class="line">    height_out = (self.<span class="built_in">input</span>.shape[<span class="number">2</span>] - self.kernel_size) // self.stride + <span class="number">1</span></span><br><span class="line">    width_out = (self.<span class="built_in">input</span>.shape[<span class="number">3</span>] - self.kernel_size) // self.stride + <span class="number">1</span></span><br><span class="line">    self.output = np.zeros([self.<span class="built_in">input</span>.shape[<span class="number">0</span>], self.<span class="built_in">input</span>.shape[<span class="number">1</span>], height_out, width_out])</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    池化计算与卷积计算很相似，但是在池化计算过程中需要输入图每个通道分开计算各自的最大值</span></span><br><span class="line"><span class="string">    然后需要记录每次最大值的index方便后续反向传播</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> idxn <span class="keyword">in</span> <span class="built_in">range</span>(self.<span class="built_in">input</span>.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> idxc <span class="keyword">in</span> <span class="built_in">range</span>(self.<span class="built_in">input</span>.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> idxh <span class="keyword">in</span> <span class="built_in">range</span>(height_out):</span><br><span class="line">                <span class="keyword">for</span> idxw <span class="keyword">in</span> <span class="built_in">range</span>(width_out):</span><br><span class="line">                    <span class="comment"># TODO： 计算最大池化层的前向传播， 取池化窗口内的最大值</span></span><br><span class="line">                    self.output[idxn, idxc, idxh, idxw] = np.<span class="built_in">max</span>(self.<span class="built_in">input</span>[idxn, idxc, idxh*self.stride:idxh*self.stride+self.kernel_size, idxw*self.stride:idxw*self.stride+self.kernel_size])</span><br><span class="line">                    curren_max_index = np.argmax(self.<span class="built_in">input</span>[idxn, idxc, idxh*self.stride:idxh*self.stride+self.kernel_size, idxw*self.stride:idxw*self.stride+self.kernel_size])</span><br><span class="line">                    curren_max_index = np.unravel_index(curren_max_index, [self.kernel_size, self.kernel_size])</span><br><span class="line">                    self.max_index[idxn, idxc, idxh*self.stride+curren_max_index[<span class="number">0</span>], idxw*self.stride+curren_max_index[<span class="number">1</span>]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> self.output</span><br></pre></td></tr></table></figure><p>反向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backward_raw_book</span>(<span class="params">self, top_diff</span>):</span><br><span class="line">    bottom_diff = np.zeros(self.<span class="built_in">input</span>.shape)</span><br><span class="line">    <span class="keyword">for</span> idxn <span class="keyword">in</span> <span class="built_in">range</span>(top_diff.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> idxc <span class="keyword">in</span> <span class="built_in">range</span>(top_diff.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">for</span> idxh <span class="keyword">in</span> <span class="built_in">range</span>(top_diff.shape[<span class="number">2</span>]):</span><br><span class="line">                <span class="keyword">for</span> idxw <span class="keyword">in</span> <span class="built_in">range</span>(top_diff.shape[<span class="number">3</span>]):</span><br><span class="line">                    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">                    应该就是让最大的梯度传回到对应位置</span></span><br><span class="line"><span class="string">                    没有看到之前记录的max index的使用，看不太懂</span></span><br><span class="line"><span class="string">                    &quot;&quot;&quot;</span></span><br><span class="line">                    max_index = np.argmax(self.<span class="built_in">input</span>[idxn, idxc, idxh*self.stride:idxh*self.stride+self.kernel_size, idxw*self.stride:idxw*self.stride+self.kernel_size])</span><br><span class="line">                    max_index = np.unravel_index(max_index, [self.kernel_size, self.kernel_size])</span><br><span class="line">                    bottom_diff[idxn, idxc, idxh*self.stride+max_index[<span class="number">0</span>], idxw*self.stride+max_index[<span class="number">1</span>]] = top_diff[idxn, idxc, idxh, idxw] </span><br><span class="line">    show_matrix(top_diff, <span class="string">&#x27;top_diff--------&#x27;</span>)</span><br><span class="line">    show_matrix(bottom_diff, <span class="string">&#x27;max pooling d_h &#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> bottom_diff</span><br></pre></td></tr></table></figure><h1 id="flatten层的实现"><a href="#flatten层的实现" class="headerlink" title="flatten层的实现"></a>flatten层的实现</h1><p>就是让输入的数据展平</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FlattenLayer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_shape, output_shape</span>):</span><br><span class="line">        self.input_shape = input_shape</span><br><span class="line">        self.output_shape = output_shape</span><br><span class="line">        <span class="keyword">assert</span> np.prod(self.input_shape) == np.prod(self.output_shape)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\tFlatten layer with input shape %s, output shape %s.&#x27;</span> % (<span class="built_in">str</span>(self.input_shape), <span class="built_in">str</span>(self.output_shape)))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">list</span>(<span class="built_in">input</span>.shape[<span class="number">1</span>:]) == <span class="built_in">list</span>(self.input_shape)</span><br><span class="line">        <span class="comment"># matconvnet feature map dim: [N, height, width, channel]</span></span><br><span class="line">        <span class="comment"># ours feature map dim: [N, channel, height, width]</span></span><br><span class="line">        self.<span class="built_in">input</span> = np.transpose(<span class="built_in">input</span>, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">        <span class="string">&quot;&quot;&quot;将每个batch都展平成self.output_shape的样子&quot;&quot;&quot;</span></span><br><span class="line">        self.output = self.<span class="built_in">input</span>.reshape([self.<span class="built_in">input</span>.shape[<span class="number">0</span>]] + <span class="built_in">list</span>(self.output_shape))</span><br><span class="line">        show_matrix(self.output, <span class="string">&#x27;flatten out &#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.output</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, top_diff</span>):</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">list</span>(top_diff.shape[<span class="number">1</span>:]) == <span class="built_in">list</span>(self.output_shape)</span><br><span class="line">        top_diff = np.transpose(top_diff, [<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">        bottom_diff = top_diff.reshape([top_diff.shape[<span class="number">0</span>]] + <span class="built_in">list</span>(self.input_shape))</span><br><span class="line">        show_matrix(bottom_diff, <span class="string">&#x27;flatten d_h &#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> bottom_diff</span><br></pre></td></tr></table></figure><p>后面会记录一下怎么对卷积操作加速</p>]]></content>
      
      
      
        <tags>
            
            <tag> 学习日常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reason for Future, Act for Now - A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency</title>
      <link href="/2024/03/23/RAFA/"/>
      <url>/2024/03/23/RAFA/</url>
      
        <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>LLM具有很强的推理能力，但是在现实世界中，将推理转变为行动依然存在着一定的问题。LLM从预训练中获得了大量的先验知识，但在现实世界中它本质上依然是无状态无根据，因此采取的决策往往不是最优的。为了解决这一问题，就使用外部反馈来迭代优化行动。</p><blockquote><p>似乎是会引入强化学习方面的知识，最近强化学习的课也在学马尔可夫决策，最优策略、最优价值什么的。看了导言说是用prompt模板作为驱动，依据外部反馈的历史信息更新prompt，再由prompt驱动决策产生和优化，感觉像这个形式，如果是这样的话其实我写的小说创作大模型的生成过程和这一框架非常地接近</p></blockquote><p>他们特别考虑了样本效率这一因素，也就是他们的目标是让智能体在与环境少交互的情况下依然能完成给定任务。</p><p>强化学习确实在引言部分被提到了，说强化学习技术与大模型由于种种原因存在一定差异</p><blockquote><p>强化学习技术在数字系统中，而大模型则是语法系统<br>大模型由通用语料库训练，强化学习则在反馈与迭代中学习</p></blockquote><p>看了一下框架图，这个框架主要由2个部分组成，一个外部memory buffer,一个agent, agent内部又包含有学习以及推理两个部分，学习是通过外部记忆缓存输入到prompt然后又大模型更新基于外界反馈的后验知识然后生成价值函数或者奖励转换模型，再将这一结果送到推理模块，依据价值函数或奖励模型产生一个树状的决策过程（？）然后选择价值最大的决策过程，并在其中选择下一步的行动立即行动，并将外部结果转存到外部记忆中。</p><blockquote><p>通过一次生成长远的决策过程，每次依据外界环境对当前决策进行优化，来避免频繁地与外部环境交互</p></blockquote><p>然后就是一些相关技术的介绍，比如大模型和in-context learning</p><blockquote><p>in-context learning就是一种让大模型仅通过观察给定的一系列例子（即上下文）来学习和执行新的任务，而无需外部的参数更新或显式的训练过程。<br>贝叶斯框架下的强化学习，大模型各种形式的推理、以及与外部环境交互的闭环框架等等，以及这些内容与RAFA的联系</p></blockquote><p>在与外部环境交互的闭环框架中，我看到文章中提到了反射框架，语言模型能够从反馈中学习来修改预先生成的轨迹的当前行为，特别是当他们犯错误时，这个论文我在课程上听到过学生进行汇报，作者特意强调了这个框架的局限性在与对局部策略的修改是短视的，忽略了对长期决策的影响。</p><blockquote><p>但我觉得这两个框架的思想是挺像的</p></blockquote><p>后续有个对于这个框架的改进用来手工避免非最优决策的产生，然后这一方法在没有足够领域知识下也很难达到最优的结果。然后作者就说了自己的框架怎么怎么好.</p><h1 id="衔接大模型和强化学习"><a href="#衔接大模型和强化学习" class="headerlink" title="衔接大模型和强化学习"></a>衔接大模型和强化学习</h1><p>首先介绍符号意思，与课上讲的马尔可夫决策过程几乎相近。最优价值、最优策略等等.</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><h2 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h2><p>与我之前讲的一致，学习模块利用大模型从buffer中学习外部环境的变化，然后给出长远规划，并由计划模块给出计划轨迹（行动决策），再将反馈存入buffer中，在外部环境发生状态转换时，LLM 代理重新调用推理例程，以从新状态重新规划另一个未来轨迹。</p><p>为了避免环境的改变可能让大模型一直在改变规划，作者设置了一个判断，就是判断一个差值，然后决定是否合并反馈信息到buffer中，在下一个改变之前一直使用之前的buffer来达到模型的稳定，具体如何判断等我往下看，这里符号我有点每太搞懂就暂时先略过去.</p><h3 id="推理未来"><a href="#推理未来" class="headerlink" title="推理未来"></a>推理未来</h3><ul><li><p><strong>Learning subroutine</strong><br>主要是将历史数据Dtk映射到转移核和值函数，用于规划子例程。Learning subroutine内部主要有两个大模型实例，一个是Model，用来生成转移核，一个是Critic，用来生成价值函数，用这两个大模型预测出可能的状态和奖励，并返回转移核Pθ和值函数Vπtθ。随着 Dtk 积累越来越多的来自外部环境的反馈,减少了关于未知环境的后验不确定性，以提高Model和Critic的准确性，从而使planning subroutine能够更准确地评估行动的长期结果。</p></li><li><p><strong>Planning subroutine</strong><br>主要是生成一个最优策略或多个未来步骤的轨迹，以最大化值函数。Planning subroutine通过模拟规划预言者PL⋆（这个是我没搞清楚的，看起来像是将后验分布参数映射到决策序列的函数）来实现这一目标。planning subroutine将Model和Critic映射到未来轨迹。Planning subroutine的任务是评估θ的最优策略π⋆θ或相应的动作at &#x3D; π⋆θ(st)，从而最大化值函数。Planning subroutine通过最大化未来累积奖励（而不是即时奖励）来返回优化的行动，从而改善长期结果。</p></li></ul><p>在整个推理未来算法介绍下来，我理解的流程是这样的，有一个外部反馈的历史缓存区Dtk，只有在满足一定条件的时候Dtk才会改变成Dtk+1，否则将一直采用Dtk以保证算法的稳定，然后在learning subroutine中有两个大模型Model和Critic，依据特殊设计的prompt与Dtk，分别生成状态转移P和价值函数V，在planning subroutine中有一个Elite大模型用来生成多种动作候选集，迭代次数U作为想要预测的未来长度，从0到U的迭代中，每次取当前状态Su由Elite生成B个动作候选加入到动作集合中，再由Model依据这些动作集合生成它们未来的状态加入到Su+1。迭代完成后，将S0、A0、、SU、AU交由Critic计算最优价值函数，然后选取使其最优的s0、a0、、su、au的序列作为预测，并返回动作a0。</p><h3 id="现在行动"><a href="#现在行动" class="headerlink" title="现在行动"></a>现在行动</h3><p>在推理未来给出了未来最优的动作与状态序列s0、a0、、su、au后，采取最优动作a0作为当前动作，并忽视后续序列，然后在下一个状态s1的时候重新规划基于s1作为初始状态的最优状态序列。</p><blockquote><p>其实就是在每个状态下都会由推理未来模块给出当前长期价值最大的状态动作序列，并采取该序列的第一个动作并执行<br>能这么做我认为是由大模型很好地预测了每个动作的下一个状态是什么，并给出了状态下最好的动作（也就是按照这个动作走在未来能取得最好的价值），这样就保证了框架能一直朝着价值最优的方向前进，即使环境会发生变化，框架也能根据变化及时调整。</p></blockquote><h1 id="理论推导"><a href="#理论推导" class="headerlink" title="理论推导"></a>理论推导</h1><p>目前还不想看，看完实验就去玩代码去</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验在文本环境中进行Game of 24, ALFWorld, BlocksWorld, and Tic-Tac-Toe.</p><blockquote><p>使用大模型在文本环境中进行比较我觉得其实，，，可能，，，用一些稍微有点设计的框架，在框架中给出prompt是让模型朝着目标行动的，（如果这个大模型性能非常好的话）都能获得一个非常好的提升，这一感想来自于多智能体课程中同学分享的反思框架，也是这篇论文之前专门提到和对比的一个框架。<br>要看一个框架是否真的在下游任务中有效我觉得要对比不同参数量的模型在使用这个框架后，与仅仅依靠prompt的模型在处理任务进行比较。</p></blockquote><p>在Game of 24中，RAFA取得了最优的成绩，RAFA和reflexion相比，RAFA从GPT3.5到GPT4提升非常明显，从{29%，46%}到{89%, 93%}（B &#x3D; {1，2},这个是之前Elite生成候选动作集的大小，当B&#x3D;1的时候生成一条链，当B&#x3D;2的时候就是生成一个二叉树），而reflexion只是从16%到21%。</p><p>ALFWorld,一个文本描述的环境提供大模型进行交互，要求大模型在该环境中完成任务，<br>总而言之是在一次迭代之后性能就优于其它框架了。</p><p>BlocksWorld,移动盒子的任务达到特定状态，有点像多智能体课程上讲的偏序规划和图规划能解决的问题。这里选用的模型是Vicuna的各种参数量级的模型，分别是13B和33B模型在4步和6步问题上进行求解，这两个模型的正确率最终其实差不多，只是一开始的正确率13B会低一点。</p><p>Tic-Tac-Toe 三子棋（井字棋）在，这个实验使用的GPT4模型，在B&#x3D;{3，4}进行实验，越多的动作候选保证了更高的胜率。</p><h1 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h1><p>刚看完实验准备吃饭去，下午看代码了，如果可以的话说不定可以用在我的小说模型上面<br>理论推导证明我都没看，有空再看，还有下面的prompt的设计（我看看也就这样）可以稍微借鉴一下。</p><p>然后就是这个框架的设计思路，将大模型与强化学习结合起来，由大模型给出状态转移、价值判断和动作候选（论文似乎没写这个大模型的prompt是如何设计的）</p><p>不过一个框架有3个大模型实例，<del>我觉得计算开销会不会非常大</del>代码里看到是调用api🥲(那对网络要求很高吧，最近一直想搞本地部署大模型，先入为主了)，而且除了环境buffer，大模型本身好像也会有历史上下文的存在，不知道如果环境buffer进行更新后大模型本身的上下文是否会对新的环境下的决策产生影响？</p><blockquote><p>我的想法是也可以使用几个大模型分别帮我生成历史文章概要、小说人物历史、未来小说大纲三个部分，并更新驱动prompt，每次迭代过程中由小说创作模型接收驱动prompt写出小说片段（类似于Dt），然后再由总结模型依据驱动prompt和当前小说片段对历史文章概要进行总结，由人物模型更新小说人物历史，再有预测模型依据前两者的结果以及小说总体走向对未来小说大纲进行一个编写</p></blockquote><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>这个实验的代码看着挺奇怪的，明明提出的是一个框架，但是这几个实验的代码几乎完全没有复用这个框架，差不多是一个实验一个代码，与我本科学习的设计模式主旨挺违背的。</p><p>代码可读性也不是很高。看着好累。</p><ol><li><p><strong>ALFWorld</strong></p><p><em><strong>这个实验都不能运行！</strong></em> 我按照他们给的环境配置方式配置了环境，但是在运行的时候一个外部库报错没有对函数的定义。</p><p>在看论文的时候就挺好奇的这个历史环境buffer是如何实现的,就先看了这部分代码，里面主要的结构就是字典列表,我对于buffer状态的改变是比较好奇的，论文中似乎对Dtk的改变情况有一个证明（我没看）。</p><blockquote><p>很奇怪的是这个历史环境buffer我只在这个实验中看到了，其它实验并没有看到对于这个类的引用或者另外的实现</p></blockquote> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnvironmentHistory</span>:</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, base_query: <span class="built_in">str</span>, start_info, memory: <span class="type">List</span>[<span class="built_in">str</span>], history: <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, <span class="built_in">str</span>]] = []</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    self._cur_query: <span class="built_in">str</span> = <span class="string">f&#x27;<span class="subst">&#123;_get_base_query(base_query, start_info, memory)&#125;</span>&#x27;</span></span><br><span class="line">    self._history: <span class="type">List</span>[<span class="type">Dict</span>[<span class="built_in">str</span>, <span class="built_in">str</span>]] = history</span><br><span class="line">    self._last_action: <span class="built_in">str</span> = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    self._is_exhausted: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p> 其中的is_exhausted就是判断状态改变的，判断依据的话我看代码是如果两次agent选择的行动相同则说明环境穷尽了需要更换策略。</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, label: <span class="built_in">str</span>, value: <span class="built_in">str</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line"><span class="keyword">assert</span> label <span class="keyword">in</span> [<span class="string">&#x27;action&#x27;</span>, <span class="string">&#x27;observation&#x27;</span>, <span class="string">&#x27;critic&#x27;</span>, <span class="string">&#x27;human_edit&#x27;</span>]</span><br><span class="line">self._history += [&#123;</span><br><span class="line">    <span class="string">&#x27;label&#x27;</span>: label,</span><br><span class="line">    <span class="string">&#x27;value&#x27;</span>: value,</span><br><span class="line">&#125;]</span><br></pre></td></tr></table></figure><p>这个类存储这四个键，后续会从这个类取值组成prompt</p><blockquote><p>其实感觉学习这个框架，更像是学习他们对于特定任务的prompt的设计</p></blockquote></li></ol><p>关于这个框架代码我觉得其实没什么看到必要，因为框架设计本身就很简单，他们对于代码的设计也就是考虑了prompt的格式以及何时将prompt喂给大模型而已。</p><p>还有关于多智能体课程上对这类大模型决策框架，有些同学就说框架做了白做对于大模型本身没有什么提升。我觉得框架的提出只是让大模型能和下游任务对齐，在某一方面更好地发挥大模型的能力而不是让大模型提升某一方面的能力，就像之前的卷积神经网络应用在各种视觉任务中，引入它们的目的都是为了让它们能对齐下游任务，发挥应有的能力而已，对于本身提升都是没有的，在卷积神经网络方面，人们还能对特定任务进行特定网络结构的设计，到大模型就要考虑计算成本而不轻易改变网络结构的。</p><p>我认为这种框架的提出算是一种启发性的方式，让人们知道如何引入大模型到决策层面的东西，将原本可能需要人来设计特定的决策范式变为现在通用大模型的特化应用，虽然大模型的训练可能硬件和时间成本非常高，但是一旦训练完就可以应用到各种领域中，而特定设计的决策范式则只能应用在某一领域中，相比较来说大模型也是具有很大的优势的，特别是在智能体领域需要个体自行决策运行具有自主性的领域（这也是为什么在过去大模型还没有提出的时候，在智能体领域方面没有特别出现令人震撼的产出，或者其实是有很让人震惊的结果但是时效性过强被淹没在智能发展的洪流中，但随着大模型的提出越来越多有趣让人振奋的应用被提出，越来越多人将大模型投入智能体领域的研究）。</p><p>当然这种框架设计我觉得可能长远来看在论文发表上是走不太远的，越设计到后面，泛用性普适性的框架可能就会越来越重要，这个时候仅仅依赖框架的设计可能回归到大模型之前的情况，引入越来越多人手工设计、引入特别多的先验知识、一直在trade-off泛用性和有效性，我觉得没有通用智能的技术是难以实现的，需要更加通用的人工智能技术像是AGI的实现，或者在决策方面专门提出一个模型能进行通用输入同时产生一种通用输出（这个应该也算是AGI了）。</p><blockquote><p>来自我的臆想</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> 大模型论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从0构建GPT</title>
      <link href="/2024/03/21/small-gpt/"/>
      <url>/2024/03/21/small-gpt/</url>
      
        <content type="html"><![CDATA[<h1 id="bigram-language-model"><a href="#bigram-language-model" class="headerlink" title="bigram language model"></a>bigram language model</h1><p>二元语言模型，仅仅关注上一个单词来预测下一个单词出现的概率。<br>不想写了感觉这个很原始</p><h1 id="small-GPT"><a href="#small-GPT" class="headerlink" title="small GPT"></a>small GPT</h1><p>这个代码从0开始构建简化GPT架构，GPT是decoder only的架构，只需要串序叠加decoder块即可。<br>看的视频是从总体框架进行的构建，再逐步细化，我觉得可以稍微学习他的编码方式</p><p>首先就是GPTModel,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTLanguageModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, block_size, n_embed, n_layer, n_head</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.block_size = block_size <span class="comment"># 一个batch中每个句子的长度</span></span><br><span class="line">        self.n_embed = n_embed</span><br><span class="line">        self.n_layer = n_layer</span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.embedding_table = nn.Embedding(vocab_size, n_embed)</span><br><span class="line">        self.position_embedding = nn.Embedding(block_size, n_embed)</span><br><span class="line">        self.blocks = nn.Sequential(*[Decoder(n_embed=n_embed, n_head=n_head) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layer)])</span><br><span class="line">        self.ln_f = nn.LayerNorm(n_embed)</span><br><span class="line">        self.lm_head = nn.Linear(n_embed, vocab_size)</span><br><span class="line"></span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, module</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, nn.Linear):</span><br><span class="line">            torch.nn.init.normal_(module.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line">            <span class="keyword">if</span> module.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                torch.nn.init.zeros_(module.bias)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(module, nn.Embedding):</span><br><span class="line">            torch.nn.init.normal_(module.weight, mean=<span class="number">0.0</span>, std=<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, _<span class="built_in">input</span>, target=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># logits是没有经过归一化的模型输出</span></span><br><span class="line">        <span class="comment"># logits = self.embedding_table(_input)</span></span><br><span class="line">        batch, time = _<span class="built_in">input</span>.shape</span><br><span class="line">        token_emb = self.embedding_table(_<span class="built_in">input</span>)</span><br><span class="line">        posi_emb = self.position_embedding(torch.arange(time, device=device))</span><br><span class="line">        <span class="comment"># print(f&quot;token_em shape = &#123;token_emb.shape&#125;, posi_em shape = &#123;posi_emb.shape&#125;&quot;)</span></span><br><span class="line">        x = token_emb + posi_emb</span><br><span class="line">        x = self.blocks(x)</span><br><span class="line">        x = self.ln_f(x)</span><br><span class="line">        logits = self.lm_head(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> target <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            batch, time, char = logits.shape</span><br><span class="line">            logits = logits.view(batch*time, char)</span><br><span class="line">            target = target.view(batch*time)</span><br><span class="line">            loss = F.cross_entropy(logits, target)</span><br><span class="line">        <span class="keyword">return</span> logits, loss</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, _<span class="built_in">input</span>, max_new_tokens</span>):</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">            <span class="comment"># 对整数索引序列进行嵌入</span></span><br><span class="line">            logits, _ = self.forward(_<span class="built_in">input</span>)</span><br><span class="line">            <span class="comment"># 代表预测仅仅关心最后一个字符</span></span><br><span class="line">            logits = logits[:, -<span class="number">1</span>, :]</span><br><span class="line">            <span class="comment"># print(f&quot;losgits shape = &#123;logits.shape&#125;&quot;)</span></span><br><span class="line">            probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># print(f&quot;probs shape = &#123;probs.shape&#125;&quot;)</span></span><br><span class="line">            _input_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">            _<span class="built_in">input</span> = torch.cat((_<span class="built_in">input</span>, _input_next), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> _<span class="built_in">input</span></span><br></pre></td></tr></table></figure><p>generate方法还是有点问，我周末再调试一下，不过现在是能训练了，打算到时候把数据集在4090那台电脑上下载一下到4090电脑上进行一个训练看看。<br>原本transformer的位置编码是固定函数，但是GPT使用可学习的嵌入进行位置编码</p><p>然后就是Decoder块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, n_head</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 每个头不需要捕获全部的特征，让每个头平均地捕获n_embed的特征</span></span><br><span class="line">        head_size = n_embed // n_head</span><br><span class="line">        self.multihead = MultiheadAttention(n_embed, n_head, head_size)</span><br><span class="line">        self.ffwd = FeedForward(n_embed)</span><br><span class="line">        self.ln1 = nn.LayerNorm(n_embed)</span><br><span class="line">        self.ln2 = nn.LayerNorm(n_embed)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x1 = self.multihead(x)</span><br><span class="line">        x = self.ln1(x + x1)</span><br><span class="line">        x2 = self.ffwd(x)</span><br><span class="line">        x = self.ln2(x + x2)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>前馈神经网络就不放了挺简单<br>然后就是多头注意力，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Head</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, head_size, dropout=<span class="number">0.2</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.key = nn.Linear(n_embed, head_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.query = nn.Linear(n_embed, head_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.value = nn.Linear(n_embed, head_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 向模型注册tril，就不用重复进行加载</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;tril&#x27;</span>, torch.tril(torch.ones(block_size, block_size)))</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 可以将time视作token</span></span><br><span class="line">        batch, time, channel = x.shape</span><br><span class="line">        k = self.key(x)</span><br><span class="line">        q = self.query(x)</span><br><span class="line">        v = self.value(x)</span><br><span class="line">        att = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * k.shape[-<span class="number">1</span>] ** -<span class="number">0.5</span> <span class="comment"># (batch, time, headsize) @ (batch, headsize, time) * dk^-0.5</span></span><br><span class="line">        att = att.masked_fill(self.tril[:time, :time] == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)) <span class="comment"># (batch, time, time)</span></span><br><span class="line">        att = F.softmax(att, dim=-<span class="number">1</span>)</span><br><span class="line">        att = self.dropout(att)</span><br><span class="line">        <span class="keyword">return</span> att @ v</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiheadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_embed, n_head, head_size, dropout=<span class="number">0.2</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># modulelist相当于一个list，可以更自由地处理list中每个模块地输入输出</span></span><br><span class="line">        self.heads = nn.ModuleList([Head(head_size) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_head)])</span><br><span class="line">        self.proj = nn.Linear(head_size * n_head, n_embed)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 将每个头按照特征维度进行组合，让每个样本的最终维度变成原来的n_embed</span></span><br><span class="line">        x = torch.cat([h(x) <span class="keyword">for</span> h <span class="keyword">in</span> self.heads], dim=-<span class="number">1</span>)</span><br><span class="line">        x = self.dropout(self.proj(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>拆解下来还比较简单，<br>下次就是训练了</p>]]></content>
      
      
      
        <tags>
            
            <tag> 大模型实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>构建大模型的尝试</title>
      <link href="/2024/03/21/page/"/>
      <url>/2024/03/21/page/</url>
      
        <content type="html"><![CDATA[<h1 id="构建大模型"><a href="#构建大模型" class="headerlink" title="构建大模型"></a>构建大模型</h1><p>终于开始要清空B站收藏夹关于大模型的视频了<br>收藏 &#x3D;&#x3D; 看过的不等式要主动划掉</p><p>在周二的时候我就看了一会<a href="https://www.bilibili.com/video/BV1vp421o7Ys?vd_source=4414e7cb68c1443baf8d60ed1c992278">《从零开始用Python搭建LLM模型|Create a LLM from Scratch with Python – Tutorial》</a></p><p>讲的很细致，很多基础的代码都会带你试验一遍。</p><p>然后在周三的时候是炉石新版本，我就没有学习，<br>新版本的触萨可以说是很有意思了，养触手到10法术之后就会只出打对面3和打10召10两张卡，非常有实力。但是稳定性挺差劲的，要想上分我觉得还是搞一个污手骑比较好。这个晚上再说，</p><p>现在就是继续看从0构建LLM争取让自己对LLM的内部有一定的了解，然后看看关于huggingface开源大模型如何构建应用扩充一下知识面。</p><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><ul><li>[ ]对自己构建地LLM进行训练</li><li>[ ]看一下课程ppt</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 学习日常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>小说大模型</title>
      <link href="/2024/03/18/page/"/>
      <url>/2024/03/18/page/</url>
      
        <content type="html"><![CDATA[<h1 id="小说生成大模型"><a href="#小说生成大模型" class="headerlink" title="小说生成大模型"></a>小说生成大模型</h1><p>很难受，初步搭建了一个小说生成的大模型框架，想法是用prompt_config来联系每一次迭代时候的大模型，在prompt_config中设置如下内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">prompt_config = &#123;</span><br><span class="line">    <span class="string">&quot;model_prompt&quot;</span>: <span class="string">&quot;Write a continuation of the novel LLM, focusing on the unfolding events and character developments.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;background&quot;</span>: <span class="string">&quot;In a dystopian future, where society is divided by technology and natural resources are dwindling,&quot;</span>,</span><br><span class="line">    <span class="string">&quot;characters&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;Alex&quot;</span>: <span class="string">&quot;a tech-savvy rebel fighting against the oppressive regime&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Jordan&quot;</span>: <span class="string">&quot;a loyalist to the regime, torn between duty and a growing sense of injustice&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Mia&quot;</span>: <span class="string">&quot;a mysterious figure with knowledge that could tip the scales of power&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;history_summary&quot;</span>: <span class="string">&quot;After a daring raid on a regime supply depot, Alex and their group find themselves pursued by an elite force led by Jordan. Amidst the chaos, they encounter Mia, who reveals a secret that could change everything.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;preview_summary&quot;</span>: <span class="string">&quot;The trio must navigate their conflicting loyalties and the dangers of a society on the brink of collapse to bring hope to the oppressed.&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>只要用这个做成一个prompt传给大模型，大模型应该就不需要非常巨大的上文检索了，因为每次都是只在prompt_config中构建prompt，每次传入的prompt应该都是差不多大小的，大模型如果首次可以生成文章，那么我觉得大模型就可以依据这个prompt_config生成任意长度的文章了。</p><p><strong>但是还是显存不够</strong></p><p>小说生成的流程主要如下：</p><ol><li>首先用户设定prompt_config，也就是故事背景和人物，已经发生的事情和将要发生的事情</li><li>然后使用prompt_config生成prompt<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">build_prompt</span>(<span class="params">prompt_config</span>):</span><br><span class="line">    prompt = prompt_config[<span class="string">&quot;model_prompt&quot;</span>]</span><br><span class="line">    prompt += <span class="string">f&quot; here is the background you need to follow, background is <span class="subst">&#123;prompt_config[<span class="string">&#x27;background&#x27;</span>]&#125;</span>,&quot;</span></span><br><span class="line">    prompt += <span class="string">f&quot; and there is a history story: <span class="subst">&#123;prompt_config[<span class="string">&#x27;history_summary&#x27;</span>]&#125;</span>.&quot;</span></span><br><span class="line">    prompt += <span class="string">f&quot; the story will going on as: <span class="subst">&#123;prompt_config[<span class="string">&#x27;preview_summary&#x27;</span>]&#125;</span>.&quot;</span></span><br><span class="line">    prompt += <span class="string">&quot; here are some characters you may use in the story&quot;</span></span><br><span class="line">    <span class="keyword">for</span> character, description <span class="keyword">in</span> prompt_config[<span class="string">&quot;characters&quot;</span>].items():</span><br><span class="line">        prompt += <span class="string">f&quot; <span class="subst">&#123;character&#125;</span> is a <span class="subst">&#123;description&#125;</span>,&quot;</span></span><br><span class="line">    <span class="comment"># Removing the last comma for proper grammar</span></span><br><span class="line">    prompt = prompt.rstrip(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> prompt</span><br></pre></td></tr></table></figure></li><li>将prompt传入模型让它进行小说生成，然后依据生成的小说总结已经发生的故事并写出未来将要写的内容<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">write_novel</span>(<span class="params">model, prompt_config, iteration_num</span>):</span><br><span class="line">    prompt = build_prompt(prompt_config)</span><br><span class="line">    novel = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(iteration_num)):</span><br><span class="line">        input_ids = tokenizer(prompt, return_tensors=<span class="string">&quot;pt&quot;</span>).input_ids</span><br><span class="line">        output = model.generate(</span><br><span class="line">            input_ids,</span><br><span class="line">            max_new_tokens=<span class="number">1000</span>,  <span class="comment"># 设定最大长度</span></span><br><span class="line">            temperature=<span class="number">0.9</span>,  <span class="comment"># 调整创造性</span></span><br><span class="line">            top_k=<span class="number">50</span>,</span><br><span class="line">            top_p=<span class="number">0.95</span>,</span><br><span class="line">            repetition_penalty=<span class="number">1.2</span>,  <span class="comment"># 减少重复内容</span></span><br><span class="line">            pad_token_id=tokenizer.eos_token_id</span><br><span class="line">        )</span><br><span class="line">        generate = tokenizer.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>) + <span class="string">&quot;\n&quot;</span></span><br><span class="line">        novel += generate</span><br><span class="line">        summ = [summarize_story(model, generate), generate_preview(model, generate)]</span><br><span class="line">        prompt_config = upgrade_prompt(summ, prompt_config)</span><br><span class="line">        prompt = build_prompt(prompt_config)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> novel</span><br></pre></td></tr></table></figure></li><li>总结文章和预测下文都使用当前迭代生成的小说正文<blockquote><p>为了避免大模型概括的一坨，总结文章就只传入了正文而没有之前的历史信息</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">summarize_story</span>(<span class="params">model, context</span>):</span><br><span class="line">    prompt = <span class="string">f&quot;there was a text that you should summarize it, you should summarize what is going on, you dont need to mention this prompt here is the text = <span class="subst">&#123;context&#125;</span>&quot;</span></span><br><span class="line">    input_ids = tokenizer(prompt, return_tensors=<span class="string">&quot;pt&quot;</span>).input_ids</span><br><span class="line">    output = model.generate(</span><br><span class="line">        input_ids,</span><br><span class="line">        max_new_tokens=<span class="number">500</span>,  <span class="comment"># 设定最大长度</span></span><br><span class="line">        temperature=<span class="number">0.3</span>,  <span class="comment"># 调整创造性</span></span><br><span class="line">        top_k=<span class="number">50</span>,</span><br><span class="line">        top_p=<span class="number">0.95</span>,</span><br><span class="line">        repetition_penalty=<span class="number">1.2</span>,  <span class="comment"># 减少重复内容</span></span><br><span class="line">        pad_token_id=tokenizer.eos_token_id</span><br><span class="line">    )</span><br><span class="line">    generated_text = tokenizer.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    split_text = <span class="string">&quot;**Summary:**&quot;</span></span><br><span class="line">    <span class="keyword">return</span> extract_summary(generated_text, split_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_preview</span>(<span class="params">model, context</span>):</span><br><span class="line">    prompt = <span class="string">f&quot;there was a text that you should tell me what will happen in the futuer, text is <span class="subst">&#123;context&#125;</span>&quot;</span></span><br><span class="line">    input_ids = tokenizer(prompt, return_tensors=<span class="string">&quot;pt&quot;</span>).input_ids</span><br><span class="line">    output = model.generate(</span><br><span class="line">        input_ids,</span><br><span class="line">        max_new_tokens=<span class="number">500</span>,  <span class="comment"># 设定最大长度</span></span><br><span class="line">        temperature=<span class="number">0.7</span>,  <span class="comment"># 调整创造性</span></span><br><span class="line">        top_k=<span class="number">50</span>,</span><br><span class="line">        top_p=<span class="number">0.95</span>,</span><br><span class="line">        repetition_penalty=<span class="number">1.2</span>,  <span class="comment"># 减少重复内容</span></span><br><span class="line">        pad_token_id=tokenizer.eos_token_id</span><br><span class="line">    )</span><br><span class="line">    generated_text = tokenizer.decode(output[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># split_text = &quot;**Summary:**&quot;</span></span><br><span class="line">    <span class="comment"># return extract_summary(generated_text, split_text)</span></span><br><span class="line">    <span class="keyword">return</span> generated_text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">upgrade_prompt</span>(<span class="params">summarize, prompt_config</span>):</span><br><span class="line">    prompt_config[<span class="string">&quot;history_summary&quot;</span>] = summarize[<span class="number">0</span>]</span><br><span class="line">    prompt_config[<span class="string">&quot;preview_summary&quot;</span>] = summarize[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> prompt_config</span><br><span class="line">    </span><br></pre></td></tr></table></figure>因为大模型的输出包括很多部分，因此需要对指定内容进行提取<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">extract_summary</span>(<span class="params">text, split_text</span>):</span><br><span class="line">    <span class="comment"># Find the position where &quot;Summary:&quot; occurs</span></span><br><span class="line">    start_index = text.find(split_text)</span><br><span class="line">    <span class="keyword">if</span> start_index != -<span class="number">1</span>:</span><br><span class="line">        <span class="comment"># Extract everything after &quot;Summary:&quot;</span></span><br><span class="line">        summary_text = text[start_index + <span class="built_in">len</span>(split_text):].strip()</span><br><span class="line">        <span class="keyword">return</span> summary_text</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;<span class="subst">&#123;split_text&#125;</span> not found.&quot;</span></span><br></pre></td></tr></table></figure></li></ol><p>明明每次传入的信息都只有一个prompt_config，但是提示输入的大小越来越大了，而且最后直接显存报不够了</p><p>不是很明白问题到底出在哪</p><p>要吃饭了我决定下次有空再优化一下，下午看看论文，看看我能不能想到别的有趣的东西做。</p><h1 id="关于一点prompt-learning和想法"><a href="#关于一点prompt-learning和想法" class="headerlink" title="关于一点prompt learning和想法"></a>关于一点prompt learning和想法</h1><p>下午也没干什么，<br>就是又在看大模型的教学视频<br>讲的是prompt learning的东西，说是这个方法在80B以上的模型进行微调会比较好<br>然后prompt有人工生成模板和自动生成模板的放法，具体我也不了解<br>我感觉要看这些东西我最好能自己手动写一个<strong>大模型出来</strong>，好像也有这方面的视频，明天开始可以看看。</p><p>这个大模型的教学视频我希望是这周能看完，关于论文什么的也就是研究方向的我希望是下个月开始渐进式看，然后这个月和下个月的目标就是看完教程和实践大模型。</p><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><ul><li>[ ]看完清华的大模型教学视频</li><li>[ ]找一些自己动手编写大模型的视频</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 大模型实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>上网的一天</title>
      <link href="/2024/03/17/page/"/>
      <url>/2024/03/17/page/</url>
      
        <content type="html"><![CDATA[<h1 id="今天本来是要休息的"><a href="#今天本来是要休息的" class="headerlink" title="今天本来是要休息的"></a>今天本来是要休息的</h1><p>昨天周六，我把周报写了，里面写的周日感冒发烧休息一天<del>反正老师也不知道我是不是真的感冒发烧</del><br>但是今天下午有个班会，我不想在寝室待到班会开始再出去又回来，所以我干脆就不休息了。</p><p>今天的计划就是尝试一下Devin 并且着手开始我的大模型微调</p><blockquote><p>是的我还是用回了原来的算力平台，并且还是4090，虽然很心痛但是要用就要用最好最安全的</p></blockquote><p>在搜索Devin的时候并没有看到能直接使用Devin，而是找到了他们官方发的blog里面有申请使用Devin的调查问卷，我就开始填。问卷里有个linkedin profile我不知道什么意思，我就去问了gpt，它说是个人简历网站，我想想以后可以写一个这个，等别人要我资料的时候我把这个链接甩过去就很帅很酷。</p><blockquote><p>用的google邮箱收Devin的反馈的，记得去看收件箱</p></blockquote><p>我还找到了一个ai广场类似的东西，专门总结一些热门的ai，<a href="https://top.aibase.com/">在这里</a></p><h1 id="开始了我的小说模型的构建"><a href="#开始了我的小说模型的构建" class="headerlink" title="开始了我的小说模型的构建"></a>开始了我的小说模型的构建</h1><p>今天看了下Fine-tuning和prompt，感觉和我之前想的似乎不太一样。<br>我就从prompt方式入手，我打算构建一个prompt_config，里面存储故事背景、人物及其性格、然后是历史故事以及未来剧情走向。</p><p>每次迭代过程中由大模型依据背景和人物和历史概要以及未来剧情走向生成新文本，然后总结新文本替代之前的历史故事，然后再由大模型根据背景故事和历史故事写一个未来剧情走向替代原本的剧情走向。</p><blockquote><p>要注意的是<strong>大模型最好注意到当前生成文本的概括与上一次迭代生成的未来剧情走向是否相一致</strong></p><p>小说会出现的人物是否会仅仅局限于已经给的角色，大模型是否可以自己构造一个新角色使得其剧情完整并填充入prompt_config中</p><p>小说片段中是否会出现全部角色，在某些场景中可以只出现部分角色，以及角色可能经历某些事件后出现性格方面的改变。</p><p>角色在经过一些事件中，角色背景应当有所改变</p><p>大模型能否注意到到当前剧情走向在主线中已经到了什么步骤</p></blockquote><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><ul><li><input checked="" disabled="" type="checkbox"> 小说大模型agent初步框架</li><li><input disabled="" type="checkbox"> GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 学习日常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>环球影城一日游</title>
      <link href="/2024/03/16/%E7%8E%AF%E7%90%83%E5%BD%B1%E5%9F%8E%E5%90%8E%E8%AE%B0/"/>
      <url>/2024/03/16/%E7%8E%AF%E7%90%83%E5%BD%B1%E5%9F%8E%E5%90%8E%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="环球影城"><a href="#环球影城" class="headerlink" title="环球影城"></a>环球影城</h1><p>3-14号去了环球影城玩，一天走了将近3W步。</p><blockquote><p>在环球影城玩让我有了触碰现实的感觉，将我从虚拟的深渊往上提了一提,每天都面对着电脑，让我对于现实时间的感知免得愚钝</p></blockquote><p>从地铁口出来之后就是安检，安检进去就是一个大的广场，广场主要是卖一些吃的和周边，感觉估计一顿饭得人均100多在这里。然后过一个桥就到了环球影城的大酒店，进去是一个商品街，这个商品街晚上开灯之后特别有感觉，特别是沿着街走到湖边的的左手边有个饭店，饭店门口停了几辆车，特别有美剧的感觉。</p><p>那天我游玩的顺序是先陪同学去坐霸天虎过山车</p><blockquote><p>这次我没有坐，有点害怕的</p></blockquote><p>然后坐了大黄蜂转圈圈，就是一个旋转的杯子在旋转的地台上，这个旋转的杯子似乎可以人手动控制旋转速度，但因为我们跟一对出来玩的老爷爷老太太坐在一个杯子里，就没有做一些很坏的事</p><p>随后去了火种争夺战，让你坐一个车子里，然后沉浸抢夺火种，不是很吓人也不是特别有意思，算是饭前甜点吧。</p><p>变形金刚出来之后去的是功夫熊猫，这块地方完全就是为小朋友准备的，玩的旋转木马啊飞天灯笼啊，玩着很舒服也很惬意，但是玩一会就没了，里面有个娱乐设施专供小朋友玩，我也想玩。</p><p>然后就去了小黄人的室内游乐园，有个小过山车和飞天旋转的和那个功夫熊猫差不多的，它可以自己控制飞天的高度，但是我们坐的后排，只能前排控制飞行高度。这个小黄人过山车是我在环球影城坐的第一个过山车，相对来说还是挺有意思的，但是转几圈就没了。</p><p>到了中午就是花车环节，拍了很多照，小黄人街区的转角特别有感觉，像在动画里一样。</p><p>然后去了未来水世界，演员都很敬业，而且爆破特效和飞机冲出来的时候观感特别特别好，飞机出来那下我还以为是特效，之后准备去哈利波特，路上被穿越侏罗纪截胡了，好玩很刺激，然后就被拉去坐霸天虎了。</p><blockquote><p>霸天虎非常刺激，好玩吗也不一定好玩，生理上折磨的项目这种过山车，刺激和爽是真的，好玩我看未必的</p></blockquote><p>然后就是爽到了，去坐了🦅🐎飞行，这个是专门拍日落的，坐完霸天虎之后园区其它的过山车类型的项目都感觉没什么感觉了，然后就是特别特别好玩的哈利波特禁忌之旅，剧情可能比较匆忙，但是整体非常好玩，还有侏罗纪大冒血，这两个都是沉浸式骑乘式的，非常有意思非常值得。</p><p>晚饭在哈利波特的餐厅吃的，118元排骨烤鸡拼盘，鸡腿闻着吃着都很香，土豆不好吃，排骨还行，就是在外面坐着吃到排骨的时候已经冷了，排骨的酱还不错，就是弄到衣服上了很难洗，玉米满分，KFC玉米，很甜也很多汁，唯一不足的是没有点饮料。</p><p>玩一天很累也很开心，晚上到地铁直接找水喝，虽然我带了水杯但是我没找到倒饮用水的地方，那个直饮水我不会用。。。。</p><p>好了，回到学校了，今天周末，要干嘛呢，写完这个写一下周报，在把明天班会要搞的什么心赏给做了，明天好好休息一天。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 学习日常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>B站OpenBMB视频</title>
      <link href="/2024/03/11/hello-huggingface/"/>
      <url>/2024/03/11/hello-huggingface/</url>
      
        <content type="html"><![CDATA[<h1 id="huggingface"><a href="#huggingface" class="headerlink" title="huggingface"></a>huggingface</h1><p>今天也是用了下huggingface，不过也遇到了很多问题。<br>一开始直接将模型页面的试用代码放上来运行了，虽然下载了很多包，但是运行碰到了没有权限access这个model<br>然后就是搜了很多很多的方法，试了几个比如代码中添加token什么的都没解决问题。</p><p>然后去看了文档，<br>首先就是要运行下面的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install huggingface_hub</span><br><span class="line">huggingface-cli login</span><br></pre></td></tr></table></figure><blockquote><p>这个会让你填入api_token，这个在个人信息那里创建一个复制过来就行，<br>不过复制到里面会显示不见，而且最好不要用Ctrl-V的方法，它提示右键复制会好一点</p></blockquote><p>然后就是下载模型，等等就行<br>7b的我16G内存运行不了，只能运行2b的，后续考虑用别的方法调用模型会好一点？🙄</p><p>在7b量化的运行中也出现了问题现在还没解决，好烦🙄</p><p>怪事，在huggingface上面运行的模型回复长度好长，本地运行的就回复几个词的<br>然后就是下载的huggingface的模型会在本地存储，存储位置在</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\78752\.cache\huggingface\hub</span><br></pre></td></tr></table></figure><p>记得删掉</p><p><strong>小小技巧</strong><br>如果希望下载的模型到你指定的位置，可以在from_pretrained方法中指定一个cache_dir参数，这个参数就是下载模型的新地址</p><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><ul><li><input checked="" disabled="" type="checkbox"> 看一下本地部署大模型的技术栈</li><li><input disabled="" type="checkbox"> 完成第一篇大模型公平性文献阅读</li><li><input checked="" disabled="" type="checkbox"> 插件更新一个保存文档就会自动部署提交</li><li><input checked="" disabled="" type="checkbox"> 图床图床图床</li><li><input disabled="" type="checkbox"> GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection?</li><li><input disabled="" type="checkbox"> Prejudice and Caprice: A Statistical Framework for Measuring Social Discrimination in Large Language Models</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 学习日常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>B站OpenBMB视频</title>
      <link href="/2024/03/11/page/"/>
      <url>/2024/03/11/page/</url>
      
        <content type="html"><![CDATA[<h1 id="啥事没干今天"><a href="#啥事没干今天" class="headerlink" title="啥事没干今天"></a>啥事没干今天</h1><p>还是看了大模型偏见与公平性的综述，继续评估标准，又看了预处理的一些东西</p><p><del>综述看的太无聊太累了</del></p><p>然后就是看了OpenBMB那个大模型知识普及的视频，从注意力到huggingface的小demo<br>大致流程也跟我之前写的huggingface一致</p><p>然后是突发奇想，想要fine-tune一个大模型专门给我写小说看。</p><p>这篇综述还是要看完的，但是我决定交给txyz来读了，到时候复制一下就行。</p><p>kaggle上用了下gemma，没搞懂，有个prompt格式，我不太理解是为什么<br>我认为下一步有需要了解训练数据或者微调数据进入大模型之后是如何训练的</p><h1 id="大模型语言排行榜"><a href="#大模型语言排行榜" class="headerlink" title="大模型语言排行榜"></a>大模型语言排行榜</h1><p>找到一个huggingface上对开源大模型的排行榜<br><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">排行榜</a></p><h1 id="关于vercel的一点点"><a href="#关于vercel的一点点" class="headerlink" title="关于vercel的一点点"></a>关于vercel的一点点</h1><p>刚刚搞了一下图床的东西<br>跟随的教程是<a href="https://www.fomal.cc/posts/d7fb1ba1.html#post-comment">博客</a><br>我的这个博客也是跟着他做的，可以说是相当厉害了</p><p>这个图床是怎么做的呢<br>首先要在github创建一个仓库，然后本地git你想要的图片上去<br>再用<a href="https://vercel.com/">vercel</a>导入刚刚的仓库<br>在项目domain中新增解析地址，使用已经有的域名进行替换<br>比如我买了一个域名xxxxxx.com什么的，我就在前面新加一个二级域名比如picpool.xxxxxxxx.com，然后我就在阿里云的DNS解析中为我的主域名添加解析，依据vercel给的配置依次填入解析项中，然后就能用这个url访问我的github的资源。</p><p>这个很有趣，相当于把GitHub仓库当作服务器存储空间，而vercel作为服务提供者为你返回资源，我想后期可以写一些api什么的，在我vscode的ai聊天或者别的插件里用用？</p><p>把QQ聊天或者甚至是阴阳师当作插件写到vscode里？<br>开发一个正大光明的摸鱼插件</p><h1 id="todo"><a href="#todo" class="headerlink" title="todo"></a>todo</h1><ul><li><input checked="" disabled="" type="checkbox"> 完成大模型公平性综述</li><li><input checked="" disabled="" type="checkbox"> 图床</li><li><input checked="" disabled="" type="checkbox"> 尝试训练一个大模型进行小说生成</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 学习日常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>智能计算系统实验一</title>
      <link href="/2024/03/09/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%AE%9E%E9%AA%8C%E4%B8%80/"/>
      <url>/2024/03/09/%E6%99%BA%E8%83%BD%E8%AE%A1%E7%AE%97%E5%AE%9E%E9%AA%8C%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络设计实验"><a href="#神经网络设计实验" class="headerlink" title="神经网络设计实验"></a>神经网络设计实验</h1><p>开始写课程作业了，努努力看看能不能一天写完，现在是早上10点45，一会吃饭去下午回来开始写，早上又在开小差什么的，下午一定认真写。</p><p>课程实验是用本地连接服务器的方式完成的，用的vscode ssh链接，用一下发现挺简单的，等我这个实验做完了我就去搞一个4090的电脑跑一下大模型玩玩看。😏</p><blockquote><p>要注意的是，链接进ssh后，需要在文件位置选择&#x2F;opt&#x2F;目录下的实验</p></blockquote><p>而且，我计划是从实验平台上将文件下载下来在本地编写后再在实验平台上评测，这样100H的机时应该可以用很久。</p><p>由于提交分数是<strong>取最后一次而不是最高分</strong>，所以记得要用git记录下版本，在最后提交最高分的那个版本即可。</p><p>下午记得看看那个代码存储在哪，不太明白这个额外创建的卷是怎么用的</p><h2 id="全连接手写数字识别"><a href="#全连接手写数字识别" class="headerlink" title="全连接手写数字识别"></a>全连接手写数字识别</h2><p>新学会一个技能，vscode要打开一个新窗口而不占用本窗口可以</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ctrl+Shift+N</span><br></pre></td></tr></table></figure><p>这个实验相当简单 <del>也不一定</del><br>我们的任务是构建一个全连接网络，一共有三种层需要我们手动实现，全连接层、ReLU层、Softmax层。每层都要实现forward和backward，</p><p>我们先看全连接层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FullyConnectedLayer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_input, num_output</span>):  <span class="comment"># 全连接层初始化</span></span><br><span class="line">        self.num_input=num_input</span><br><span class="line">        self.num_output=num_output</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\tFully connected layer with input %d, output %d.&#x27;</span> % (self.num_input, self.num_output))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_param</span>(<span class="params">self, std=<span class="number">0.01</span></span>):  <span class="comment"># 参数初始化</span></span><br><span class="line">        self.weight = np.random.normal(loc=<span class="number">0.0</span>, scale=std, size=(self.num_input, self.num_output))</span><br><span class="line">        self.bias=np.zeros([<span class="number">1</span>, self.num_output])</span><br><span class="line">        show_matrix(self.weight, <span class="string">&#x27;fc weight &#x27;</span>)</span><br><span class="line">        show_matrix(self.bias, <span class="string">&#x27;fc bias &#x27;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>): <span class="comment"># 前向传播计算</span></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        self.<span class="built_in">input</span> = <span class="built_in">input</span></span><br><span class="line">        <span class="comment"># TODO：全连接层的前向传播，计算输出结果</span></span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        self.output = np.dot(self.<span class="built_in">input</span>, self.weight) + self.bias</span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        <span class="keyword">return</span> self.output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, top_diff</span>):   <span class="comment"># 反向传播的计算</span></span><br><span class="line">        <span class="comment"># TODO：全连接层的反向传播，计算参数梯度和本层损失</span></span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        self.d_weight = np.dot(self.<span class="built_in">input</span>.T, top_diff)</span><br><span class="line">        self.d_bias = np.<span class="built_in">sum</span>(top_diff, axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        bottom_diff = np.dot(top_diff, self.weight.T)</span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        <span class="keyword">return</span> bottom_diff</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_gradient</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.d_weight,self.d_bias</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_param</span>(<span class="params">self, lr</span>):  <span class="comment"># 参数更新</span></span><br><span class="line">        <span class="comment"># TODO：对全连接层参数利用参数进行更新</span></span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        self.weight = self.weight - lr * self.d_weight</span><br><span class="line">        self.bias = self.bias - lr * self.d_bias</span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_param</span>(<span class="params">self, weight, bias</span>): <span class="comment"># 参数加载</span></span><br><span class="line">        <span class="keyword">assert</span> self.weight.shape == weight.shape</span><br><span class="line">        <span class="keyword">assert</span> self.bias.shape == bias.shape</span><br><span class="line">        self.weight=weight</span><br><span class="line">        self.bias=bias</span><br><span class="line">        show_matrix(self.weight, <span class="string">&#x27;fc weight &#x27;</span>)</span><br><span class="line">        show_matrix(self.bias, <span class="string">&#x27;fc bias &#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_param</span>(<span class="params">self</span>):    <span class="comment"># 参数保存</span></span><br><span class="line">        show_matrix(self.weight, <span class="string">&#x27;fc weight &#x27;</span>)</span><br><span class="line">        show_matrix(self.bias, <span class="string">&#x27;fc bias &#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.weight, self.bias</span><br></pre></td></tr></table></figure><p>forward和参数更新都相当简单，主要搞清楚矩阵是如何相乘的，<br>首先就是输入x，维度为1 * input_dim，然后就是权重矩阵input_num * output_num<br>偏置就是1 * output_num<br>重点就是backward如何计算</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.d_weight = np.dot(self.input.T, top_diff)</span><br></pre></td></tr></table></figure><p>为什么权重是这样子的。<br>从全连接公式说起<br>每一层的公式是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Y` = W.T * X + b</span><br><span class="line">维度分别是</span><br><span class="line">Y`_(1*out_num)</span><br><span class="line">W_(input_num*output_num)</span><br><span class="line">X_(1*input_num)</span><br><span class="line">b_(1*1)</span><br><span class="line"></span><br><span class="line">损失公式去均方误差</span><br><span class="line">L = 0.5(Y - Y`)^2</span><br><span class="line"></span><br><span class="line">损失是权重和偏置的函数，要求对权重和偏置的偏导</span><br><span class="line">就要用到链式法则，先求对Y`的导数再求Y`对权重或偏置的导数</span><br><span class="line"></span><br><span class="line">此时dL/dY` = -(Y - Y`), 这个也就是top_diff记作ΔLy,形状是(1*output_num)</span><br><span class="line">dY`/dW = X,形状是(1*input_num)</span><br><span class="line">此时dL/dW = ΔLy * X</span><br><span class="line">显然这么写肯定不能乘的</span><br><span class="line">考虑到要对权重进行更新，因此dL/dW = X.T * ΔLy，这也就是为什么代码要这么写的。</span><br></pre></td></tr></table></figure><blockquote><p>为什么会标注矩阵形状就是因为我不知道有的公式为什么矩阵可以这么相乘</p></blockquote><p>其它层也都是同理的分析方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReLULayer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t Relu layer&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):  <span class="comment"># 前向传播的计算</span></span><br><span class="line">        start_time = time.time()</span><br><span class="line">        self.<span class="built_in">input</span>=<span class="built_in">input</span></span><br><span class="line">        <span class="comment"># TODO：ReLU层的前向传播，计算输出结果</span></span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        output = np.maximum(<span class="number">0</span>, self.<span class="built_in">input</span>)</span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">self, top_diff</span>):   <span class="comment"># 反向传播的计算</span></span><br><span class="line">        <span class="comment"># TODO：ReLU层的反向传播，计算本层损失</span></span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        bottom_diff = top_diff * (self.<span class="built_in">input</span> &gt; <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># ==============================================================================================</span></span><br><span class="line">        <span class="keyword">return</span> bottom_diff</span><br></pre></td></tr></table></figure><p>backward为什么这么实现，是因为relu层本身不含有参数，它作为滤网将非零结果前向和反向传播，因此只需要让梯度乘上一个mask让非0梯度传回即可。</p><blockquote><p>这里，self.input &gt; 0会产生一个布尔数组，其中大于0的输入位置是True（对应于1），其余是False（对应于0）。将这个数组与top_diff相乘，就可以实现只有当输入大于0时，误差梯度才会传回</p></blockquote><p>剩下softmax层我不想说了有点累睡午觉了，不过挺简单的。<del>我其实不知道为什么backward为什么这么写</del> 下次再说</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class SoftmaxLossLayer(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        print(&#x27;\tSoftmax loss layer.&#x27;)</span><br><span class="line">    def forward(self, input):  # 前向传播的计算</span><br><span class="line">        # TODO：softmax 损失层的前向传播，计算输出结果</span><br><span class="line">        input_max = np.max(input, axis=1, keepdims=True)</span><br><span class="line">        input_exp = np.exp(input-input_max)</span><br><span class="line">        exp_sum = np.sum(input_exp, axis=1, keepdims=True)</span><br><span class="line">        # ==============================================================================================</span><br><span class="line">        self.prob = input_exp / exp_sum</span><br><span class="line">        # ==============================================================================================</span><br><span class="line">        return self.prob</span><br><span class="line"></span><br><span class="line">    def get_loss(self,label):  # 计算损失</span><br><span class="line">        self.batch_size=self.prob.shape[0]</span><br><span class="line">        self.label_onehot=np.zeros_like(self.prob)</span><br><span class="line">        self.label_onehot[np.arange(self.batch_size),label]=1.0</span><br><span class="line">        loss=-np.sum(np.log(self.prob)*self.label_onehot)/self.batch_size</span><br><span class="line">        return loss</span><br><span class="line">    def backward(self):   # 反向传播的计算</span><br><span class="line">        # TODO：softmax 损失层的反向传播，计算本层损失</span><br><span class="line">        # ==============================================================================================</span><br><span class="line">        bottom_diff = (self.prob - self.label_onehot) / self.batch_size</span><br><span class="line">        # ==============================================================================================</span><br><span class="line">        return bottom_diff</span><br></pre></td></tr></table></figure><p>然后就是读测试数据啊构建模型什么的这也都相当简单。</p><h2 id="DLP实验平台"><a href="#DLP实验平台" class="headerlink" title="DLP实验平台"></a>DLP实验平台</h2><blockquote><p>我快笑死了，这个实验只要把模型搭建起来就行了，要满分的条件是DLP运行时间是CPU运行时间的五十分之一，哈哈哈哈我直接让CPU代码里的layer层每层都加了个sleep，直接给我干满分了</p></blockquote><p>这个没什么好讲的复制粘贴改下参数就行。<br>也不是很想学这个加速以后要用再说<br>要使用这个pycnnl库首先就是要</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install swig</span><br></pre></td></tr></table></figure><p>然后在目录下运行pycnnl文件夹里面</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./build_pycnnl.sh</span><br></pre></td></tr></table></figure><p>好像是这个我名字忘记了好了不写了我要休息了</p><p>突然找到了一个学长的实验答案，虽然可能代码有变动但是有一定的参考价值<br><a href="https://github.com/doongz/aics/tree/main">智能计算系统</a></p><p>找到了一个完全能用的答案应该是<a href="https://github.com/LuoXukun/AI-homework">答案</a></p><h1 id="MD"><a href="#MD" class="headerlink" title="MD"></a>MD</h1><p>无语死了，昨天好好的ssh用着突然用不了，点击连接主机没有反应，就是错误没有提示也没有。</p><p>试了重新加载vscode，重新启动，没有用<br>我就去问了GPT，它给了好多解决方案，我就一个一个去试了试，因为没有任何错误提示我也很无从下手的。</p><p>经过几个方案之后错误依然没有任何好转，完全就是正常使用的时候突然无法使用还不告诉你为什么不能使用，我挺无奈。</p><p>gpt就告诉我可以打开vscode开发者模式可以看运行过程中有没有错误在控制栏中给出但是并没用用提示框提示用户，然后我就打开了，里面确实出现了一个错误和一个警告</p><blockquote><p>【warning】An iframe which has both allow-scripts and allow-same-origin for its sandbox attribute can escape its sandboxing.</p><p>【error】Cannot read properties of undefined (reading ‘after’): Error: Cannot read properties of undefined (reading ‘after’)<br>at l.h (d:\Microsoft VS Code\resources\app\out\vs\workbench\api\node\extensionHostProcess.js:150:185186)</p></blockquote><p>因为错误并没有出现在extensions文件夹里，我就以为可能不是插件的问题，可能是vscode本体因为我可能操作顺序问题导致了不给弹窗，</p><p>然后我就到处搜这两个问题，完全没有搜到跟我这个相关。反正时间花费了很久。</p><p>我就又去问gpt了，它就告诉我可以清空本地扩展缓存什么的，清除重新加载vscode再下载插件排除问题，因为就ssh一个插件是我要实验的，就这么清除缓存和下载vscode搞了几次还是不行，我就又去搜索相关问题了，翻翻看可能会不会有别的相似的问题能解决这个问题。</p><p>反正就是搞了好久，差点就要去定位源码一个一个去分析了，我把插件版本回退之后就又好了，无语死了也不知道这次会不会用着用着就用出问题。</p><p>等我把实验都拿满分了就把我的实验过程给补完，这种从0开始搭建网络的还是比较有趣的。</p><p><strong>最好是把插件更新一个保存一个文件就自动提交部署这个功能给更新了</strong></p><h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><ul><li><input checked="" disabled="" type="checkbox"> 更新插件让它能在保存时自动提交</li><li><input checked="" disabled="" type="checkbox"> 搞个图床，找一些高清图像资源，把博客背景和文章封面什么的都换了</li><li><input checked="" disabled="" type="checkbox"> 完成第一篇大模型公平性文献阅读</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 学习日常 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bias and Fairness in Large Language Models A Survey</title>
      <link href="/2024/03/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%81%8F%E8%A7%81%E4%B8%8E%E5%85%AC%E5%B9%B3%E6%80%A7/"/>
      <url>/2024/03/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%81%8F%E8%A7%81%E4%B8%8E%E5%85%AC%E5%B9%B3%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<h1 id="公平性与偏见"><a href="#公平性与偏见" class="headerlink" title="公平性与偏见"></a>公平性与偏见</h1><p>首先就是大模型的背景，大模型在大语料库的训练下具有很强的零或少样本的学习能力，能在很多下游任务上取得很好的效果，但正因如此，大语料库隐含的偏见有可能被大模型捕捉而学习，从而影响下游任务产生偏见。</p><p>这篇文章主要从三个方面介绍</p><ul><li>评估矩阵</li><li>评估数据集</li><li>去偏算法</li></ul><p>文章的贡献有</p><ul><li>NLP 的社会偏见和公平定义的巩固、形式化和扩展</li><li>偏差评估指标的调查和分类</li><li>用于偏差评估的数据集的调查和分类，以及公开数据集的汇编<blockquote><p><a href="https://github.com/i-gallegos/Fair-LLM-Benchmark">公开数据集</a></p></blockquote></li><li>减轻偏见技术的调查和分类</li></ul><h2 id="公式化大模型偏见与公平性"><a href="#公式化大模型偏见与公平性" class="headerlink" title="公式化大模型偏见与公平性"></a>公式化大模型偏见与公平性</h2><ol><li>对大模型的定义</li></ol><p>文章将大模型定义为：由 θ 参数化的大型语言模型 (LLM) M 是一种基于 Transformer 的模型，具有自回归、自动编码或编码器-解码器架构，已在包含数亿到数万亿个标记的大型语料库上进行了训练。</p><ol start="2"><li><p>对大模型偏见的定义</p><ul><li>社会偏见与公平性<blockquote><p>这里有讲到当一个标签被贴到一个群体上使得群体的边界合法化，此时就强调了这个群体与其它群体之间的差异，而这个差异则会导致强化社会等级制度和权力失衡，通常会带来非常真实和物质的后果，可能导致隔离、边缘化和压迫。</p></blockquote></li></ul><p> 社会群体的定义</p><blockquote><p>具有相同身份特征的人口子集，该特征可能是固定的、特定背景的或社会构建的。</p></blockquote><p> 受保护属性的定义</p><blockquote><p>确定社会群体身份的共享特征</p></blockquote><p> 群体公平性</p><blockquote><p>要求任意两个群体之间以群体身份进行测量的结果应当是相似的</p></blockquote><p> 但显然一个人并不一定只属于一个群体，当一个人可能属于多个群体的时候，单个群体的公平性可能很难保证个人的公平性<br> 因此个人公平性的定义</p><blockquote><p>原文中的定义是在某些任务中相似的人应当获得相似的待遇</p></blockquote><p> 但是我看公式的意思应该是模型对任意两个人的输出差异应当小于这两个人原本的差异</p><p> 社会偏见来源于由于历史和结构性权力不对称而产生的社会群体之间不同待遇和结果，这可能就导致NLP模型的各种直接歧视和间接歧视</p><ul><li>NLP任务中的偏见</li></ul><p> 论文在这里论述了一堆，说了很多关于语言在偏见方面的能力。然后就介绍到在NLP中会出现的偏见</p><p> 文本生成体现的局部偏见和总体偏见</p><blockquote><p>局部偏见就是一个句子中对相近词语的替换，比如男性换成女性，然后来评估模型结果的差异。<br>总体偏见就是全部句子体现的模型对于某一个群体的情绪</p></blockquote><p> 机器翻译中可能会默认使用男性词汇作为标准，比如我很快乐让模型进行转述时候模型会用他很快乐而不是她</p><p> 信息检索中即使使用非性别指导的查询，模型依然会返回更多的男性相关的文档。</p><p> 问答系统的偏见可能在于在大规模的语料库预训练之后捕捉到了人们对某一群体的偏见，而在回答问题中体现这些偏见。</p><p> 分类任务中，模型可能会将部分群体的内容分类为负面结果</p><p> 自然语言推理中的偏见问题</p><blockquote><p>在自然语言处理中，自然语言推理（NLI）是一项重要的任务，旨在判断两个句子之间的逻辑关系。这些逻辑关系通常分为三种：蕴含（entailment）、矛盾（contradiction）和中立（neutral）。蕴含意味着第一个句子（前提）逻辑上支持第二个句子（假设）的真实性；矛盾意味着前提与假设在逻辑上不可能同时为真；中立则是指前提与假设之间没有直接的逻辑关系。</p><p>这段话提出的问题是，在进行自然语言推理时，一些模型可能会依赖于错误的表述或者刻板印象来做出推断，这可能会导致不准确或无效的推断结果。例如，在判断“会计吃了一个百吉饼”与“那个男人吃了一个百吉饼”或“那个女人吃了一个百吉饼”的关系时，理论上这两个句子之间的关系应该是中性的，因为“会计”这一身份与性别无关，不应该影响到句子的逻辑关系。然而，如果模型有偏见，它可能错误地认为这两个句子之间存在蕴含或矛盾关系，这表明模型在推理过程中受到了刻板印象的影响，没有正确处理信息，从而导致了错误的推断结果。</p></blockquote><ul><li>开发与部署生命周期中的偏见</li></ul><p> 训练数据可能只来自部分群体，而不能很好地利用在其它群体上；数据可能无法区分全部群体，而在需要对部分群体区别对待的基础上进行笼统概括</p><p> 模型中的偏差可能在于，训练和推理过程中放大了偏差；同时优化函数的不同选择如公平性或准确性的偏向也会影响模型；对训练样本的权衡，或者模型输出的排序也会影响模型偏差。</p><p> 基准数据集可能不能囊括所有使用大模型的群体，因此可以对已经囊括的群体进行优化</p><p> 大模型部署的环境可能和设定不同</p></li><li><p>大模型公平性的需要</p></li></ol><p>传统机器学习的公平性定义难以满足NLP任务的公平性需求，因此需要更多的定义来帮助实现NLP的公平性，此处见原文，我不会打公式。</p><p>然后罗列了一堆文章介绍到的各种公平性评估和公平性方法。</p><h2 id="偏差评估标准的分类"><a href="#偏差评估标准的分类" class="headerlink" title="偏差评估标准的分类"></a>偏差评估标准的分类</h2><p>在评估模型偏差过程中包含了几个层面：任务指向、偏差类型、数据结构、指标输入。</p><ol><li>基于替代的公平性评估</li></ol><p>通过比较文本输入是性别互换之后的句子，依据模型给出的结果进行差异比较</p><ol start="2"><li>基于用途的指标分类</li></ol><p>对大模型公平性分类的指标可以依据模型使用的东西如嵌入、概率、文本生成等进行分类。</p><ul><li><p>基于嵌入的指标<br>  词嵌入：很直观就是指代某一群体的特殊向量与中性词的余弦距离应当都一致而不能偏向一方<br>  句嵌入：相对于词嵌入，句子层级的嵌入更适合大模型，可以更有效地评估多维度的偏差</p><blockquote><p>关于基于嵌入的指标，文章中提到如果违反了社会对于某些群体的语言使用被违反，那么受保护属性的表示与其它词语的关联与下游任务表现的差异就完全独立了，就是有没有了受保护属性与其它词语的关联都没有关系。而且基于嵌入的评价指标可能高度依赖不同的设计选择，因此最好能避免使用，而是专注于特定下游任务的指标评估。</p></blockquote></li><li><p>基于概率的指标</p><ol><li>掩码token</li></ol><p>  就是给两个句子，句子主题是两个不同的社会群体，然后用预测词的概率分布差异来比较模型如何对待两个社会群体。</p><ol start="2"><li>伪对数似然</li></ol><p>  给定两个完整的句子，两个句子含有不同的社会群体，评估模型在给定每个单词的条件概率下，比较选择最终两个句子的可能。</p><blockquote><p>mask一个token，然后用其它没有被mask的token来预测这个token，用来近似条件概率</p></blockquote><ol start="3"><li>对数概率偏差分数LPBS</li></ol><p>  这个就是比较两个不同社会群体在用先验标准化的情况下，两个群体分别出现在一个含有中性词语句子中的对数概率差异</p><p>4. </p><p>  好无聊综述看的<br>  真没意思<br>  看看别的去好了</p><p>  缺陷与嵌入评估相似，似乎是和下游任务很少相关</p></li><li><p>基于文本生成的指标</p></li></ul><p>看图似乎是给定一个prompt然后让大模型生成，通过评判生成结果来评估公平性</p><h2 id="偏见缓解技术"><a href="#偏见缓解技术" class="headerlink" title="偏见缓解技术"></a>偏见缓解技术</h2><ul><li><p>预处理</p><ul><li><p>数据增强</p><blockquote><p>add new examples which is not biased to the training data</p></blockquote><p>看图也就是改变训练数据中的性别，就是反事实训练一样，让社会群体互相替代来训练模型，来达到平衡训练数据的目的。数据平衡的做法一般是让样本成对出现、或者让对立样本在训练集中平衡</p><blockquote><p>缺陷在哪里：受到不完整的单词对列表的限制，并且在交换术语时可能会引入语法错误。</p></blockquote></li><li><p>数据过滤与重新加权<br>过滤掉很有攻击性和偏见的数据，让那些现实世界受到偏见群体的样本权重增加，降低不受到偏见群体的样本权重</p></li><li><p>数据生成</p><blockquote><p>上面两种方法的缺陷在于需要识别每个偏差维度的示例，这些偏差可能会根据上下文、应用程序或所需行为而有所不同。<br>相似句子替换，也就是用更高质量的训练数据进行训练</p></blockquote></li><li><p>指令微调<br>在每个样本之前添加一个基调让模型以这个基调学习这个样本</p></li><li><p>基于投影的缓解<br>貌似是识别与保护属性有关的嵌入空间，然后改变上下文嵌入以清除偏见维度</p></li></ul></li><li><p>训练中缓解</p><ul><li>结构修改<br>修改模型配置，利用适配器添加新的可训练参数</li><li>损失函数修改<br>用损失函数作为新指标，来平衡对立社会群体的属性偏见</li><li>选择参数更新<br>冻结大部分预训练参数而选择少部分参数进行更新，如果不冻结就有可能在微调过程中忘记预训练的内容，称为灾难性遗忘。</li><li>过滤模型参数<br>冻结全部参数，但以去偏为目标剪枝部分参数</li></ul></li><li><p>处理中缓解</p><ul><li>受约束的next token search<br>限制下一个有偏见或冒犯意味的词语，或者将社会中受偏见的群体标签替换上来</li><li>改变token分布<br>降低对立社会群体中未受偏见的群体标签的分布，增加受偏见一方的分布，同时大大减少包含偏见意味的词语</li><li>权重重分配<br>对注意力矩阵修改，避免某个社会标签与偏见token有过多的相关</li><li>模块化去偏网络<br>独立的去偏模块帮助LLM去除偏见</li></ul></li><li><p>后处理</p><ul><li>重写<br>重写策略使用基于规则或神经的重写算法来检测有害单词，并用更积极或更具代表性的术语替换它们。该策略考虑完全生成的输出（与解码技术中的下一个单词预测相反）。</li></ul><blockquote><p>后处理缓解措施不假设可以访问可训练模型，这使得这些技术适用于黑盒模型。也就是说，重写技术本身很容易表现出偏见。决定重写哪些输出本身就是一个主观且充满价值的决定。与毒性和情感分类器的潜在危害类似（参见第 3.5.4 节），应特别注意确保某些社会群体的语言风格不会被过度标记和改写。删除受保护的属性还可能抹去重要的背景并产生较少多样性的输出，这本身就是一种排他性规范的形式。神经重写器还受到并行训练语料库可用性的限制，这可能会限制它们要解决的偏见的维度。</p></blockquote></li></ul><h2 id="问题与挑战"><a href="#问题与挑战" class="headerlink" title="问题与挑战"></a>问题与挑战</h2><ul><li><p>解决权力不平衡</p><ul><li><p>将边缘群体中心化</p><blockquote><p>等级制度可能会加深偏见，同时在消除偏见的过程中，消除偏见的方法也可能会引入新的偏见，技术人员在进行去偏时候必须辩证看待问题，更广泛地打破有害的权力失衡的核心是将边缘化社区带入法学硕士决策和系统开发的最前沿，</p></blockquote></li><li><p>开发参与性研究设计<br>就是走群众路线，引入社区人员的意见</p><blockquote><p>将社区成员纳入研究过程，以更好地理解和代表他们的需求。更广泛地说，建立社区循环研究框架可以打破技术人员和受影响社区之间的权力不平衡。</p></blockquote></li><li><p>重建价值观和假设<br>我感觉这个不是特别现实</p><blockquote><p>偏见和公平是社会、文化、历史、政治和地区背景下高度主观和规范的概念。因此，偏见和公平研究无法假设一套单一的价值观</p></blockquote><p>但其实看完这一段，作者的意思应该是使用更广阔背景的训练数据来训练大模型，因为在某一特定文化环境下训练的大模型可能在另一个文化环境中使用的时候表现比较强的偏见。</p><blockquote><p>科学和计算研究中的假设和价值观往往反映了主导群体的假设和价值观。研究人员和实践者可以建立更严格的社会变革理论，以语言学、社会学和哲学等领域的相关原理为基础，</p></blockquote></li><li><p>额外语言资源</p><p>由上一条可知，需要别的语言的优秀数据集，同时保证数据集的质量</p></li></ul></li><li><p>NLP公平性</p><ul><li><p>对公平性扩展的迫切需要，目前机器学习已经有一套公平性和偏见的框架，但是在自然语言任务中应当有扩展的更具体的公平性</p></li><li><p>重新思考社会群体的定义。通常需要对社会群体进行划分来评估差异，但同时也可以使社会结构合法化，强化权力差异，并促成压迫制度</p></li></ul><p>就是说现有的社会群体架构可能会过于局限，不能考虑到更细粒度的群体而隐式地导致对细粒度群体的偏见</p><ul><li>承认不同的社会群体<br>这一点我是认可的，因为如果单纯采用消去社会群体的保护属性以期望模型公平对待不同的群体这是不太现实的，偏见来源是多样的，单纯消去保护属性可能会使得更加无缘故的偏见加在某一社会群体上。应当针对现实社会群体的偏见原因，合理制定去偏的方案</li></ul></li><li><p>优化评价准则<br>字面意思</p></li><li><p>改进缓解措施</p><ul><li><p>实现可扩展性。</p><p>这个也就是说，现有的缓解偏见的方法与特定算法耦合，使得去偏的范围变得狭隘，比如说列个避免词表啊、针对部分偏见进行优化这种，此时模型对于缓解的偏见就局限在这一部分，而其它部分的偏见就难以缓解，要想更好应对更多的偏见就需要去偏方法有更大的扩展能力。</p></li><li><p>混合技术</p><p>用多种阶段的公平性算法来去偏，但我觉得这可能会让模型性能更弱</p></li><li><p>大模型对偏见的编码，在哪个组件，去偏算法如何缓解这些等问题都悬而未决</p></li></ul></li><li><p>探索理论极限</p><ul><li>性能与公平的trade off</li><li>理论化模型<br>现有理论来自于经验</li></ul></li></ul><h1 id="终于TM看完了，真不想看了"><a href="#终于TM看完了，真不想看了" class="headerlink" title="终于TM看完了，真不想看了"></a>终于TM看完了，真不想看了</h1><p>我要搞一个gemma玩玩看的，最好微调一下再给我写个小说什么的。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 大模型论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>llama2cpp</title>
      <link href="/2024/03/05/llama2cpp/"/>
      <url>/2024/03/05/llama2cpp/</url>
      
        <content type="html"><![CDATA[<h1 id="本地部署llama2"><a href="#本地部署llama2" class="headerlink" title="本地部署llama2"></a>本地部署llama2</h1><p>根据B站视频攻略做的部署，<a href="https://www.bilibili.com/video/BV1m34y1M7dM?vd_source=4414e7cb68c1443baf8d60ed1c992278">无需GPU，本地部署Llama2-13B模型</a></p><ol><li>首先就是要在windows环境下安装wsl，这个是模拟linux环境的<br>查看有哪些linux系统可以进行安装<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --list --online</span><br></pre></td></tr></table></figure></li><li>复制你想安装的系统名称进行安装<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --install -d sys_name</span><br></pre></td></tr></table></figure>再根据要求创建用户名密码就行</li><li>更换存储区域，默认下载的wsl是下载在C盘，需要更换一下 ~~我的C盘不多了。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wsl -l -v # 显示已经安装了的系统</span><br><span class="line">wsl --export sys_name target_dir # sys_name=你的系统命， target_dir就是目标路径，我的是D：\Ubuntu-22.04.tar</span><br><span class="line">wsl --unregister sys_name # 将原来的删除</span><br><span class="line">wsl --import sys_name source_dir target_dir --version 2 # source_dir就是你希望它存储的位置</span><br></pre></td></tr></table></figure></li><li>启动！<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wsl --distribution sys_name --user user_name # user_name是之前设置的用户名</span><br></pre></td></tr></table></figure></li><li>在wsl中下载<a href="https://docs.anaconda.com/free/miniconda/index.html">miniconda</a><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><blockquote><p>得等个二十分钟左右，我就是在等的时候写的这个东西。。😭</p></blockquote></li></ol><p>下载完成后</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>然后阅读协议，一直回车然后yesyesyes就完成了安装</p><ol start="6"><li><p>刷新环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc # 会进入base环境</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ # 添加conda镜像</span><br><span class="line">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 添加pip镜像</span><br></pre></td></tr></table></figure></li><li><p>下载llama和llama.cpp，然后进入llama</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/facebookresearch/llama.git</span><br><span class="line">git clone https://github.com/ggerganov/llama.cpp.git</span><br><span class="line">cd llama</span><br><span class="line">ll</span><br></pre></td></tr></table></figure><p>进去之后可以看到有专门下载模型的脚本，然后要去官网获取下载链接</p></li></ol><h1 id="夭折！！"><a href="#夭折！！" class="headerlink" title="夭折！！"></a>夭折！！</h1><p>下载LLAMA模型的表单提交不了！提示“There was an error submitting your email address.”</p><p>算了， 用huggingface试试google gemma？</p>]]></content>
      
      
      
        <tags>
            
            <tag> 大模型实践 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vscode插件 hexo_helper</title>
      <link href="/2024/03/04/page/"/>
      <url>/2024/03/04/page/</url>
      
        <content type="html"><![CDATA[<h1 id="my-first-blog"><a href="#my-first-blog" class="headerlink" title="my first blog"></a>my first blog</h1><p>在学习过程中突然有了记笔记的想法，这样可以时时记录学习过程，免得导师问这周干了什么的时候只能擦汗，于是就想到了搭建个人博客，用日记记录日常学习的过程。<br>因为文档生成有一个固定的模板，方便博客框架对文档信息进行渲染显示，因此为了避免每次都需要复制粘贴这个模板，我就用自动化脚本的方式来完成这一过程。</p><span id="more"></span><h2 id="保存时运行脚本"><a href="#保存时运行脚本" class="headerlink" title="保存时运行脚本"></a>保存时运行脚本</h2><p>vscode中下载run on save插件，并在setting.json中文件进行设置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;emeraldwalk.runonsave&quot;: &#123;</span><br><span class="line">    &quot;commands&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;match&quot;: &quot;\\.md$&quot;,</span><br><span class="line">            &quot;cmd&quot;: &quot;python $&#123;workspaceFolder&#125;/your_script_path.py $&#123;file&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这样就能匹配所有md文件，在保存过程中调用python脚本，<br>python脚本是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取当前日期和时间</span></span><br><span class="line">current_datetime = datetime.now().strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>)</span><br><span class="line">file_path = sys.argv[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_or_insert_updated_field</span>(<span class="params">content, current_datetime</span>):</span><br><span class="line">    <span class="comment"># 检测是否已存在updated字段，并准备相应的正则表达式</span></span><br><span class="line">    updated_pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;updated: [\d-]+\s[\d:]+&#x27;</span>)</span><br><span class="line">    date_pattern = re.<span class="built_in">compile</span>(<span class="string">r&#x27;date: [\d-]+\s[\d:]+&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> updated_pattern.search(content):</span><br><span class="line">        <span class="comment"># 如果存在updated字段，则更新其值</span></span><br><span class="line">        updated_content = updated_pattern.sub(<span class="string">f&#x27;updated: <span class="subst">&#123;current_datetime&#125;</span>&#x27;</span>, content)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果不存在updated字段，找到date字段并在其下插入updated字段</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">replacement</span>(<span class="params"><span class="keyword">match</span></span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="string">f&quot;<span class="subst">&#123;<span class="keyword">match</span>.group(<span class="number">0</span>)&#125;</span>\nupdated: <span class="subst">&#123;current_datetime&#125;</span>&quot;</span></span><br><span class="line">        updated_content = date_pattern.sub(replacement, content)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> updated_content</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r+&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    content = file.read()</span><br><span class="line">    updated_content = update_or_insert_updated_field(content, current_datetime)</span><br><span class="line">    file.seek(<span class="number">0</span>)</span><br><span class="line">    file.write(updated_content)</span><br><span class="line">    file.truncate()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>它能自动获取md文档，并在文档顶部的模板中更新updated时间<br>使用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page_name</span><br></pre></td></tr></table></figure><p>就能新建一个含有原始文档模板的文件</p><h2 id="vscode扩展插件"><a href="#vscode扩展插件" class="headerlink" title="vscode扩展插件"></a>vscode扩展插件</h2><p>乐坏了，上面的脚本不用了，我直接用gpt4，0基础从0开发了vscode插件。</p><blockquote><p>gpt4写起来很快，调试过程很漫长<br>而且没学过typescript，也没开发过vscode插件，测试代码非常非常困难<br>生成一个能初步完成要求的插件发布插件只有和gpt4交谈半个多小时，解决bug得要浪费一个下午</p></blockquote><p>步骤大概如下</p><ol><li>首先需要确保有node.js和vscode</li><li>运行下列代码来创建插件骨架<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install -g yo generator-code</span><br><span class="line">yo code</span><br></pre></td></tr></table></figure></li><li>然后就是代码内容了，在extension.ts中编写就行了<blockquote><p>具体见<a href="https://github.com/zuoqiumama/hexo_blog_helper">github</a></p></blockquote></li></ol><p>其中出现比较多的问题就是正则匹配的问题，不过出现什么问题把问题描述一遍给gpt4听就完事了。</p><ol start="4"><li>最后就是打包</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vsce package</span><br></pre></td></tr></table></figure><p>然后就能在vscode的插件市场的右上角以vsix的方式进行安装，就能使用了。</p><p>目前的功能只有在hexo项目下对新建的md文件插入顶部模板的作用，后续可能还会再加吧</p><p>累一天没多大收获的想法就是：可以学学prompt更好地来用gpt4</p><p>还要学习下typescript，感觉写插件什么的还挺有意思的，<br>下次想写一个自己的代码生成插件什么的</p><h1 id="hexo-helper-update"><a href="#hexo-helper-update" class="headerlink" title="hexo_helper update"></a>hexo_helper update</h1><p>现在插件可以在文件保存之后自动提交文档了，美妙的了<br>主要的逻辑就是我默认了如果文章有了标题之后就可以进行部署</p><p>代码也比较简单就是在保存的时候直接提交命令就行<br>代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">function deployHexoBlog() &#123;</span><br><span class="line">    <span class="keyword">return</span> new Promise((resolve, reject) =&gt; &#123;</span><br><span class="line">        //const options = &#123; cwd: <span class="string">&#x27;D:\\hexo_blog\\blog&#x27;</span> &#125;; // 替换为您的Hexo根目录</span><br><span class="line">        <span class="built_in">exec</span>(<span class="string">&#x27;cd D:\\hexo_blog\\blog &amp; hexo cl &amp; hexo g &amp; hexo d&#x27;</span>, (error, stdout, stderr) =&gt; &#123;</span><br><span class="line">            console.log(<span class="string">&#x27;stdout:&#x27;</span>, stdout);</span><br><span class="line">            console.log(<span class="string">&#x27;stderr:&#x27;</span>, stderr);</span><br><span class="line">            <span class="keyword">if</span> (error) &#123;</span><br><span class="line">                vscode.window.showErrorMessage(`🥵 Something wrong <span class="keyword">in</span> deploying blog: $&#123;error&#125;`, &#123; modal: true &#125;);</span><br><span class="line">                console.error(`<span class="built_in">exec</span> error: $&#123;error&#125;`);</span><br><span class="line">                reject(error);</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            vscode.window.showInformationMessage(<span class="string">&#x27;🥳 Yes!Blog deployed successfully!&#x27;</span>);</span><br><span class="line">            resolve(stdout);</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>全是GPT的功劳！<br>现在可以说我的插件该有名字了，也算是我的一个小项目<br>后期更新的话可能会引入配置文件了是，让模板和tags都能在配置中进行更改<br>再进一步可以引申到更多的文件类型和模板！🥰</p><h1 id="todo-list"><a href="#todo-list" class="headerlink" title="todo list"></a>todo list</h1><ul><li><input checked="" disabled="" type="checkbox"> 保存文件时弹出窗口选择文章tags</li><li><input checked="" disabled="" type="checkbox"> ~~完成一篇大模型公平性文献阅读 收集几篇大模型公平性论文</li><li><input checked="" disabled="" type="checkbox"> B站视频本地部署llama2 ~~下载不了模型纯纯小问题</li><li><input checked="" disabled="" type="checkbox"> 更新插件让它能在保存时自动提交</li><li><input checked="" disabled="" type="checkbox"> 搞个图床，找一些高清图像资源，把博客背景和文章封面什么的都换了</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 学习日常 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
